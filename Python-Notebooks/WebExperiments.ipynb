{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib2 import urlopen\n",
    "endpoint = \"http://www.tigerjython.ch\"\n",
    "endpoint = \"http://python.org\"\n",
    "response = urlopen(endpoint)\n",
    "html = response.read()\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET HTML mit Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib2 import urlopen\n",
    "endpoint = \"http://www.tigerjython.ch/index.php?inhalt_links=navigation.inc.php&inhalt_mitte=robotik/lernendeRobot.inc.php\"\n",
    "endpoint = \"http://www.tigerjython.ch\"\n",
    "response = urlopen(endpoint)\n",
    "html = response.read()\n",
    "encodedhtml = unicode(html, 'iso-8859-1')\n",
    "#print(html)\n",
    "print(encodedhtml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET RESPONSE.Headers & HTTP STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib2 import urlopen\n",
    "\n",
    "conn = urlopen(\"http://www.tigerjython.ch\")\n",
    "status = conn.getcode()\n",
    "reason = conn.msg\n",
    "\n",
    "print(status,reason)\n",
    "print(con.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib2 import urlopen\n",
    "\n",
    "endpoint = \"http://www.tigerjython.ch\"\n",
    "endpoint = \"http://www.tigerjython.ch/index.php?inhalt_links=navigation.inc.php&inhalt_mitte=lernumgebung/lernumgebung.inc.php\"\n",
    "endpoint = \"http://pdf.tigerjython.ch\"\n",
    "\n",
    "handler = urlopen(endpoint)\n",
    "headers = handler.headers\n",
    "code = handler.getcode()\n",
    "msg = handler.msg\n",
    "handler.close()\n",
    "\n",
    "print(\"HTTP-Status:\",code)\n",
    "print(msg)\n",
    "print(headers)\n",
    "print handler.headers['content-type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## JSON Informationen über github User mgje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib2 import urlopen\n",
    "\n",
    "endpoint = \"https://api.github.com/users/mgje\"\n",
    "handler = urlopen(endpoint)\n",
    "\n",
    "j_obj = json.loads(handler.read())\n",
    "\n",
    "userid = j_obj['login']\n",
    "name = j_obj['name']\n",
    "created_at = j_obj['created_at']\n",
    "\n",
    "print('GitHub user ',userid)\n",
    "print('Name: ',name)\n",
    "print('since: ',created_at)\n",
    "\n",
    "handler.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TigerJython Beispiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2, json\n",
    "\n",
    "search = \"tigerjython+lernen\"\n",
    "url = \"http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\" + search\n",
    "responseStr = urllib2.urlopen(url).read()\n",
    "response = json.loads(responseStr)\n",
    "\n",
    "#print \"response:\\n\" + str(response) + \"\\n\"\n",
    "\n",
    "responseData = response[\"responseData\"]\n",
    "#print \"reponseData:\\n\" + str(responseData) + \"\\n\"\n",
    "\n",
    "results = responseData[\"results\"]\n",
    "#print \"results:\\n\" + str(results) + \"\\n\"\n",
    "\n",
    "for result in results:\n",
    "    title = result[\"title\"]\n",
    "    url = result[\"url\"]\n",
    "    print title + \" ---- \" + url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Parsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "from urllib2 import urlopen\n",
    "\n",
    "# create a subclass and override the handler methods\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        print \"Encountered a start tag:\", tag\n",
    "    def handle_endtag(self, tag):\n",
    "        print \"Encountered an end tag :\", tag\n",
    "    def handle_data(self, data):\n",
    "        print \"Encountered some data  :\", data\n",
    "\n",
    "\n",
    "endpoint = \"http://www.tigerjython.ch\"\n",
    "response = urlopen(endpoint)\n",
    "html = response.read()\n",
    "print(html)\n",
    "\n",
    "# instantiate the parser and fed it some HTML\n",
    "parser = MyHTMLParser()\n",
    "parser.feed(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more complex Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "from htmlentitydefs import name2codepoint\n",
    "from urllib2 import urlopen\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        print \"Start tag:\", tag\n",
    "        for attr in attrs:\n",
    "            print \"     attr:\", attr\n",
    "    def handle_endtag(self, tag):\n",
    "        print \"End tag  :\", tag\n",
    "    def handle_data(self, data):\n",
    "        print \"Data     :\", data\n",
    "    def handle_comment(self, data):\n",
    "        print \"Comment  :\", data\n",
    "    def handle_entityref(self, name):\n",
    "        c = unichr(name2codepoint[name])\n",
    "        print \"Named ent:\", c\n",
    "    def handle_charref(self, name):\n",
    "        if name.startswith('x'):\n",
    "            c = unichr(int(name[1:], 16))\n",
    "        else:\n",
    "            c = unichr(int(name))\n",
    "        print \"Num ent  :\", c\n",
    "    def handle_decl(self, data):\n",
    "        print \"Decl     :\", data\n",
    "\n",
    "        \n",
    "endpoint = \"http://www.tigerjython.ch\"\n",
    "response = urlopen(endpoint)\n",
    "html = response.read()\n",
    "print(html)\n",
    "\n",
    "parser = MyHTMLParser()\n",
    "parser.feed(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nach bestimmten Inhalten Parsen\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "\n",
    "\n",
    "class AllLanguages(HTMLParser):\n",
    "    def __init__(self):\n",
    "        HTMLParser.__init__(self)\n",
    "        self.inLink = False\n",
    "        self.dataArray = []\n",
    "        self.countLanguages = 0\n",
    "        self.lasttag = None\n",
    "        self.lastname = None\n",
    "        self.lastvalue = None\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        self.inLink = False\n",
    "        if tag == 'a':\n",
    "            for name, value in attrs:\n",
    "                if name == 'class' and value == 'Vocabulary':\n",
    "                    self.countLanguages += 1\n",
    "                    self.inLink = True\n",
    "                    self.lasttag = tag\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == \"a\":\n",
    "            self.inlink = False\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if self.lasttag == 'a' and self.inLink and data.strip():\n",
    "            print data\n",
    "\n",
    "\n",
    "parser = AllLanguages()\n",
    "parser.feed(\"\"\"\n",
    "<html>\n",
    "<head><title>Test</title></head>\n",
    "<body>\n",
    "<a href=\"http://wold.livingsources.org/vocabulary/1\" title=\"Swahili\" class=\"Vocabulary\">Swahili</a>\n",
    "<a href=\"http://wold.livingsources.org/contributor#schadebergthilo\" title=\"Thilo Schadeberg\" class=\"Contributor\">Thilo Schadeberg</a>\n",
    "<a href=\"http://wold.livingsources.org/vocabulary/2\" title=\"English\" class=\"Vocabulary\">English</a>\n",
    "<a href=\"http://wold.livingsources.org/vocabulary/2\" title=\"Russian\" class=\"Vocabulary\">Russian</a>\n",
    "</body>\n",
    "</html>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Parser Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dodo(v):\n",
    "    b = 33\n",
    "    if v == 2:\n",
    "        c = 4\n",
    "        d = 8\n",
    "    print vars()\n",
    "\n",
    "  \n",
    "dodo(1)\n",
    "dodo(2)\n",
    "\n",
    "attrs =[['href','www'],['blbla','dada']]\n",
    "\n",
    "tloc = map(lambda x: 1 if x[0]=='href' else 0,attrs)\n",
    "attr_loc = tloc.index(1)\n",
    "print tloc\n",
    "print attr_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "print bcolors.WARNING + \"Warning: No active frommets remain. Continue?\" + bcolors.ENDC\n",
    "\n",
    "CSI=\"\\x1B[\"\n",
    "\n",
    "print CSI+\"30;45m\" + \"Colored Text\" + CSI + \"0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "from urllib2 import urlopen\n",
    "\n",
    "class MyParse(HTMLParser):\n",
    "    def __init__(self):\n",
    "        #super() does not work for this class\n",
    "        HTMLParser.__init__(self)\n",
    "        self.tag_stack = []\n",
    "        self.attr_stack = []\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        #take the tag off the stack if it matches the next close tag\n",
    "        #if you are expecting unmatched tags, then this needs to be more robust\n",
    "        if self.tag_stack[len(self.tag_stack)-1][0] == tag:\n",
    "            self.tag_stack.pop()\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        #'data' is the text between tags, not necessarily\n",
    "        #matching tags\n",
    "        #this gives you a link to the last tag\n",
    "        if len(self.tag_stack) > 0:\n",
    "            tstack = self.tag_stack[len(self.tag_stack)-1]\n",
    "        else:\n",
    "            tstack = 0\n",
    "            \n",
    "        print(tstack,data)\n",
    "        #do something with the text\n",
    "            \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        #add tag to the stack\n",
    "        self.tag_stack.append([tag, attrs])\n",
    "        #if this tag is a link\n",
    "        if tag ==\"a\":\n",
    "            #these next few lines find if there is a hyperlink in the tag\n",
    "            tloc = map(lambda x: 1 if x[0]=='href' else 0,attrs)\n",
    "            try:\n",
    "                #did we find any hyperlinks\n",
    "                attr_loc = tloc.index(1)\n",
    "            except:\n",
    "                pass\n",
    "            # attr_loc only exists if we found a hyperlink\n",
    "            if vars().has_key('attr_loc'):\n",
    "                #append to the last item in the stack the location of the hyperlink\n",
    "                #note, this does not increase the length of the stack\n",
    "                #as we are putting it inside the last item on the stack\n",
    "                self.tag_stack[len(self.tag_stack)-1].append(attr_loc)\n",
    "\n",
    "endpoint = \"http://curioussystem.com\"\n",
    "response = urlopen(endpoint)\n",
    "html = response.read()\n",
    "#print(html)\n",
    "                \n",
    "p = MyParse()\n",
    "p.feed(html)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "from urllib2 import urlopen\n",
    "\n",
    "\n",
    "class LinksExtractor(HTMLParser): # derive new HTML parser\n",
    "\n",
    "    def __init__(self) :        # class constructor\n",
    "        HTMLParser.__init__(self)  # base class constructor\n",
    "        self.links = []        # create an empty list for storing hyperlinks\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag != 'a':\n",
    "            return\n",
    "        else:\n",
    "            if len(attrs) > 0 :\n",
    "                for attr in attrs :\n",
    "                    if attr[0] == \"href\" :         # ignore all non HREF attributes\n",
    "                        self.links.append(attr[1])\n",
    "            \n",
    "        \n",
    "    def get_links(self) :     # return the list of extracted links\n",
    "        return self.links\n",
    "    \n",
    "\n",
    "htmlparser = LinksExtractor() \n",
    "\n",
    "endpoint = \"http://www.tigerjython.ch/index.php?inhalt_links=navigation.inc.php&inhalt_mitte=internet/search.inc.php\"\n",
    "\n",
    "links = htmlparser.get_links()   # get the hyperlinks list\n",
    "\n",
    "#Print only external Links\n",
    "for link in links:\n",
    "    s = link[:7]\n",
    "    if s.lower() == 'http://':\n",
    "        print link + \"\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple httplib request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from httplib import HTTPConnection\n",
    "conn = HTTPConnection(\"www.python.org\")\n",
    "conn.request(\"GET\",\"/\")\n",
    "res = conn.getresponse()\n",
    "print res.status, res.reason\n",
    "conn.close()\n",
    "\"\"\"\n",
    "\n",
    "from httplib import HTTPConnectionb\n",
    "conn = HTTPConnection(\"www.tigerjython.ch\")\n",
    "conn.request(\"GET\",\"/index.html\")\n",
    "res = conn.getresponse()\n",
    "print res.status, res.reason\n",
    "for header in res.getheaders():\n",
    "    print(header[0]+\" : \"+header[1])\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if externallink works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "from urllib2 import urlopen\n",
    "from httplib import HTTPConnection, HTTPException\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "class LinksExtractor(HTMLParser): # derive new HTML parser\n",
    "\n",
    "    def __init__(self) :        # class constructor\n",
    "        HTMLParser.__init__(self)  # base class constructor\n",
    "        self.links = []        # create an empty list for storing hyperlinks\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag != 'a':\n",
    "            return\n",
    "        else:\n",
    "            if len(attrs) > 0 :\n",
    "                for attr in attrs :\n",
    "                    if attr[0] == \"href\" :         # ignore all non HREF attributes\n",
    "                        self.links.append(attr[1])\n",
    "            \n",
    "        \n",
    "    def get_links(self) :     # return the list of extracted links\n",
    "        return self.links\n",
    "    \n",
    "\n",
    "htmlparser = LinksExtractor() \n",
    "\n",
    "endpoint = \"http://www.tigerjython.ch/index.php?inhalt_links=navigation.inc.php&inhalt_mitte=anhang/links.inc.php\"  \n",
    "#endpoint = \"http://www.tigerjython.ch/index.php?inhalt_links=navigation.inc.php&inhalt_mitte=internet/search.inc.php\"\n",
    "\n",
    "response = urlopen(endpoint)\n",
    "html = response.read()\n",
    "response.close()\n",
    "\n",
    "encodedhtml=unicode(html, 'iso-8859-1')\n",
    "htmlparser.feed(encodedhtml)      # parse the file saving the info about links\n",
    "htmlparser.close()\n",
    "links = htmlparser.get_links()   # get the hyperlinks list\n",
    "\n",
    "# Color String\n",
    "CSI=\"\\x1B[\"\n",
    "\n",
    "#links=[\"http://www.unibas.ch\",\"http://www.blick.ch\",\"http://www.basel.ch\"]\n",
    "\n",
    "for link in links:\n",
    "    s = link[:7]\n",
    "    if s.lower() == 'http://':\n",
    "        url = link[7:]\n",
    "        if len(url.split('/'))>1:\n",
    "            site,path = url.split('/',1)\n",
    "            path = '/'+path\n",
    "        else:\n",
    "            url2 = url.split('/',1)\n",
    "            site = url2[0]\n",
    "            path = \"/\"\n",
    "            \n",
    "        conn = HTTPConnection(site,timeout=3)\n",
    "        try:\n",
    "            conn.request(\"HEAD\",path)\n",
    "            res = conn.getresponse()\n",
    "            conn.close()\n",
    "            if res.status != 200:\n",
    "                print CSI+\"30;45m\" + res.reason + \" \" + link +  CSI + \"0m\"+\"\\n\"\n",
    "            else:\n",
    "                print CSI+\"30;42m\" +\"OK: \"+CSI+\"30;42m\"+ link + CSI + \"0m\"+\"\\n\"\n",
    "        except:\n",
    "            print  CSI+\"30;45m\" + \"could not connect \" + link +   CSI + \"0m\"+\"\\n\"\n",
    "        \n",
    "        sys.stdout.flush()\n",
    "                \n",
    "\"END\"\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check URLS with urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "from urllib2 import urlopen\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "class LinksExtractor(HTMLParser): # derive new HTML parser\n",
    "\n",
    "    def __init__(self) :        # class constructor\n",
    "        HTMLParser.__init__(self)  # base class constructor\n",
    "        self.links = []        # create an empty list for storing hyperlinks\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag != 'a':\n",
    "            return\n",
    "        else:\n",
    "            if len(attrs) > 0 :\n",
    "                for attr in attrs :\n",
    "                    if attr[0] == \"href\" :         # ignore all non HREF attributes\n",
    "                        self.links.append(attr[1])\n",
    "            \n",
    "        \n",
    "    def get_links(self) :     # return the list of extracted links\n",
    "        return self.links\n",
    "    \n",
    "\n",
    "htmlparser = LinksExtractor() \n",
    "\n",
    "endpoint = \"http://www.tigerjython.ch/index.php?inhalt_links=navigation.inc.php&inhalt_mitte=anhang/links.inc.php\"  \n",
    "#endpoint = \"http://www.tigerjython.ch/index.php?inhalt_links=navigation.inc.php&inhalt_mitte=internet/search.inc.php\"\n",
    "\n",
    "response = urlopen(endpoint)\n",
    "html = response.read()\n",
    "response.close()\n",
    "\n",
    "encodedhtml=unicode(html, 'iso-8859-1')\n",
    "htmlparser.feed(encodedhtml)      # parse the file saving the info about links\n",
    "htmlparser.close()\n",
    "links = htmlparser.get_links()   # get the hyperlinks list\n",
    "\n",
    "# Color String\n",
    "CSI=\"\\x1B[\"\n",
    "\n",
    "#links=[\"http://www.unibas.ch\",\"http://www.blick.ch\",\"http://www.basel.ch\"]\n",
    "\n",
    "for link in links:\n",
    "    s = link[:7]\n",
    "    if s.lower() == 'http://':\n",
    "        try:\n",
    "            conn = urlopen(link,timeout=3)\n",
    "            code = conn.getcode()\n",
    "            msg = conn.msg\n",
    "            conn.close()\n",
    "            if code != 200:\n",
    "                print CSI+\"30;45m\" + msg + \" \" + link +  CSI + \"0m\"+\"\\n\"\n",
    "            else:\n",
    "                print CSI+\"30;42m\" +\"OK: \"+CSI+\"30;42m\"+ link + CSI + \"0m\"+\"\\n\"\n",
    "        except:\n",
    "            print  CSI+\"30;45m\" + \"could not connect \" + link +   CSI + \"0m\"+\"\\n\"\n",
    "        \n",
    "        sys.stdout.flush()\n",
    "                \n",
    "\"END\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2, json\n",
    "\n",
    "search = input(\"Enter a search string(AND-connect with +):\")\n",
    "url = \"http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\" + search\n",
    "responseStr = urllib2.urlopen(url).read()\n",
    "response = json.loads(responseStr)\n",
    "\n",
    "#print \"response:\\n\" + str(response) + \"\\n\"\n",
    "\n",
    "responseData = response[\"responseData\"]\n",
    "#print \"reponseData:\\n\" + str(responseData) + \"\\n\"\n",
    "\n",
    "results = responseData[\"results\"]\n",
    "#print \"results:\\n\" + str(results) + \"\\n\"\n",
    "\n",
    "for result in results:\n",
    "    title = result[\"title\"]\n",
    "    url = result[\"url\"]\n",
    "    print title + \" ---- \" + url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"Web Crawler/Spider\n",
    "\n",
    "This module implements a web crawler. This is very _basic_ only\n",
    "and needs to be extended to do anything usefull with the\n",
    "traversed pages.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import urllib2\n",
    "import urlparse\n",
    "import optparse\n",
    "from cgi import escape\n",
    "from traceback import format_exc\n",
    "from Queue import Queue, Empty as QueueEmpty\n",
    "\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "\n",
    "__version__ = \"0.2\"\n",
    "__copyright__ = \"CopyRight (C) 2008-2011 by James Mills\"\n",
    "__license__ = \"MIT\"\n",
    "__author__ = \"James Mills\"\n",
    "__author_email__ = \"James Mills, James dot Mills st dotred dot com dot au\"\n",
    "\n",
    "USAGE = \"%prog [options] <url>\"\n",
    "VERSION = \"%prog v\" + __version__\n",
    "\n",
    "AGENT = \"%s/%s\" % (__name__, __version__)\n",
    "\n",
    "class Crawler(object):\n",
    "\n",
    "    def __init__(self, root, depth, locked=True):\n",
    "        self.root = root\n",
    "        self.depth = depth\n",
    "        self.locked = locked\n",
    "        self.host = urlparse.urlparse(root)[1]\n",
    "        self.urls = []\n",
    "        self.links = 0\n",
    "        self.followed = 0\n",
    "\n",
    "    def crawl(self):\n",
    "        page = Fetcher(self.root)\n",
    "        page.fetch()\n",
    "        q = Queue()\n",
    "        for url in page.urls:\n",
    "            q.put(url)\n",
    "        followed = [self.root]\n",
    "\n",
    "        n = 0\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                url = q.get()\n",
    "            except QueueEmpty:\n",
    "                break\n",
    "\n",
    "            n += 1\n",
    "\n",
    "            if url not in followed:\n",
    "                try:\n",
    "                    host = urlparse.urlparse(url)[1]\n",
    "                    if self.locked and re.match(\".*%s\" % self.host, host):\n",
    "                        followed.append(url)\n",
    "                        self.followed += 1\n",
    "                        page = Fetcher(url)\n",
    "                        page.fetch()\n",
    "                        for i, url in enumerate(page):\n",
    "                            if url not in self.urls:\n",
    "                                self.links += 1\n",
    "                                q.put(url)\n",
    "                                self.urls.append(url)\n",
    "                        if n > self.depth and self.depth > 0:\n",
    "                            break\n",
    "                except Exception, e:\n",
    "                    print \"ERROR: Can't process url '%s' (%s)\" % (url, e)\n",
    "                    print format_exc()\n",
    "\n",
    "class Fetcher(object):\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.urls = []\n",
    "\n",
    "    def __getitem__(self, x):\n",
    "        return self.urls[x]\n",
    "\n",
    "    def _addHeaders(self, request):\n",
    "        request.add_header(\"User-Agent\", AGENT)\n",
    "\n",
    "    def open(self):\n",
    "        url = self.url\n",
    "        try:\n",
    "            request = urllib2.Request(url)\n",
    "            handle = urllib2.build_opener()\n",
    "        except IOError:\n",
    "            return None\n",
    "        return (request, handle)\n",
    "\n",
    "    def fetch(self):\n",
    "        request, handle = self.open()\n",
    "        self._addHeaders(request)\n",
    "        if handle:\n",
    "            try:\n",
    "                content = unicode(handle.open(request).read(), \"utf-8\",\n",
    "                        errors=\"replace\")\n",
    "                soup = BeautifulSoup(content)\n",
    "                tags = soup('a')\n",
    "            except urllib2.HTTPError, error:\n",
    "                if error.code == 404:\n",
    "                    print >> sys.stderr, \"ERROR: %s -> %s\" % (error, error.url)\n",
    "                else:\n",
    "                    print >> sys.stderr, \"ERROR: %s\" % error\n",
    "                tags = []\n",
    "            except urllib2.URLError, error:\n",
    "                print >> sys.stderr, \"ERROR: %s\" % error\n",
    "                tags = []\n",
    "            for tag in tags:\n",
    "                href = tag.get(\"href\")\n",
    "                if href is not None:\n",
    "                    url = urlparse.urljoin(self.url, escape(href))\n",
    "                    if url not in self:\n",
    "                        self.urls.append(url)\n",
    "\n",
    "def getLinks(url):\n",
    "    page = Fetcher(url)\n",
    "    page.fetch()\n",
    "    for i, url in enumerate(page):\n",
    "        print \"%d. %s\" % (i, url)\n",
    "\n",
    "def parse_options():\n",
    "    \"\"\"parse_options() -> opts, args\n",
    "\n",
    "    Parse any command-line options given returning both\n",
    "    the parsed options and arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    parser = optparse.OptionParser(usage=USAGE, version=VERSION)\n",
    "\n",
    "    parser.add_option(\"-q\", \"--quiet\",\n",
    "            action=\"store_true\", default=False, dest=\"quiet\",\n",
    "            help=\"Enable quiet mode\")\n",
    "\n",
    "    parser.add_option(\"-l\", \"--links\",\n",
    "            action=\"store_true\", default=False, dest=\"links\",\n",
    "            help=\"Get links for specified url only\")\n",
    "\n",
    "    parser.add_option(\"-d\", \"--depth\",\n",
    "            action=\"store\", type=\"int\", default=30, dest=\"depth\",\n",
    "            help=\"Maximum depth to traverse\")\n",
    "\n",
    "    opts, args = parser.parse_args()\n",
    "\n",
    "    if len(args) < 1:\n",
    "        parser.print_help()\n",
    "        raise SystemExit, 1\n",
    "\n",
    "    return opts, args\n",
    "\n",
    "def main():\n",
    "    opts, args = parse_options()\n",
    "\n",
    "    url = args[0]\n",
    "\n",
    "    if opts.links:\n",
    "        getLinks(url)\n",
    "        raise SystemExit, 0\n",
    "\n",
    "    depth = opts.depth\n",
    "\n",
    "    sTime = time.time()\n",
    "\n",
    "    print \"Crawling %s (Max Depth: %d)\" % (url, depth)\n",
    "    crawler = Crawler(url, depth)\n",
    "    crawler.crawl()\n",
    "    print \"\\n\".join(crawler.urls)\n",
    "\n",
    "    eTime = time.time()\n",
    "    tTime = eTime - sTime\n",
    "\n",
    "    print \"Found:    %d\" % crawler.links\n",
    "    print \"Followed: %d\" % crawler.followed\n",
    "    print \"Stats:    (%d/s after %0.2fs)\" % (\n",
    "            int(math.ceil(float(crawler.links) / tTime)), tTime)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib2 import urlopen, URLError\n",
    "\n",
    "fdurl = urlopen(\"http://www.unibas.ch\",timeout=2)\n",
    "#realsock = fdurl.fp._sock.fp._sock* # we want to close the \"real\" socket later\n",
    "realsock = fdurl.fp\n",
    "#req = Request(url, header)\n",
    "print dir(realsock)\n",
    "print realsock.__doc__\n",
    "print realsock.closed\n",
    "realsock.close()\n",
    "print realsock.closed\n",
    "try:\n",
    "    fdurl = urllib2.urlopen(\"http://www.tigerjython.ch\",timeout=2)\n",
    "    print fdurl.read()\n",
    "except urllib2.URLError,e:\n",
    "    print \"urlopen exception\", e\n",
    "    realsock.close() \n",
    "    fdurl.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import Queue\n",
    "import urllib2\n",
    "\n",
    "# utility - spawn a thread to execute target for each args\n",
    "def run_parallel_in_threads(target, args_list):\n",
    "    result = Queue.Queue()\n",
    "    # wrapper to collect return value in a Queue\n",
    "    def task_wrapper(*args):\n",
    "        result.put(target(*args))\n",
    "    threads = [threading.Thread(target=task_wrapper, args=args) for args in args_list]\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    return result\n",
    "\n",
    "def dummy_task(n):\n",
    "    for i in xrange(n):\n",
    "        time.sleep(0.1)\n",
    "    return n\n",
    "\n",
    "# below is the application code\n",
    "urls = [\n",
    "    ('http://www.google.com/',),\n",
    "    ('http://www.lycos.com/',),\n",
    "    ('http://www.bing.com/',),\n",
    "    ('http://www.altavista.com/',),\n",
    "    ('http://achewood.com/',),\n",
    "]\n",
    "\n",
    "def fetch(url):\n",
    "    return urllib2.urlopen(url).read()\n",
    "\n",
    "q =  run_parallel_in_threads(fetch, urls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Parsen mit BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "\n",
    "response = urllib2.urlopen(\"http://www.pythonscraping.com/pages/page3.html\")\n",
    "html = response.read()\n",
    "bsObj = BeautifulSoup(html)\n",
    "\n",
    "for sibling in bsObj.find(\"table\",{\"id\":\"giftList\"}).tr.next_siblings:\n",
    "    print(sibling) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "images = bsObj.findAll(\"img\", {\"src\":re.compile(\"\\.\\.\\/img\\/gifts/img.*\\.jpg\")})\n",
    "for image in images: \n",
    "    print(image[\"src\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WikiPedia by Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import random\n",
    "import re\n",
    "\n",
    "random.seed(datetime.datetime.now())\n",
    "\n",
    "def getLinks(articleUrl):\n",
    "    res = urllib2.urlopen(\"http://de.wikipedia.org\"+articleUrl)\n",
    "    html = res.read()\n",
    "    bsObj = BeautifulSoup(html)\n",
    "    return bsObj.find(\"div\", {\"id\":\"bodyContent\"}).findAll(\"a\", href=re.compile(\"^(/wiki/)((?!:).)*$\"))\n",
    "\n",
    "links = getLinks(\"/wiki/Basel\")\n",
    "while len(links) > 15:\n",
    "    newArticle = links[random.randint(0, len(links)-1)].attrs[\"href\"]\n",
    "    print(newArticle)\n",
    "    links = getLinks(newArticle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib2\n",
    "\n",
    "\n",
    "def getCountry(ipAddress):\n",
    "    response = urllib2.urlopen(\"http://freegeoip.net/json/\"+ipAddress).read().decode('utf-8')\n",
    "    responseJson = json.loads(response)\n",
    "    return responseJson.get(\"country_code\")\n",
    "\n",
    "print(getCountry(\"50.78.253.58\"))\n",
    "print(getCountry(\"131.152.1.1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiki History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import random\n",
    "import re\n",
    "\n",
    "random.seed(datetime.datetime.now())\n",
    "def getLinks(articleUrl):\n",
    "    res = urllib2.urlopen(\"http://de.wikipedia.org\"+articleUrl)\n",
    "    html = res.read()\n",
    "    bsObj = BeautifulSoup(html)\n",
    "    return bsObj.find(\"div\", {\"id\":\"bodyContent\"}).findAll(\"a\", \n",
    "                     href=re.compile(\"^(/wiki/)((?!:).)*$\"))\n",
    "\n",
    "def getHistoryIPs(pageUrl):\n",
    "    #Format of revision history pages is: \n",
    "    #http://en.wikipedia.org/w/index.php?title=Title_in_URL&action=history\n",
    "    pageUrl = pageUrl.replace(\"/wiki/\", \"\")\n",
    "    historyUrl = \"http://de.wikipedia.org/w/index.php?title=\"+pageUrl+\"&action=history\"\n",
    "    print(\"history url is: \"+historyUrl)\n",
    "    res = urllib2.urlopen(historyUrl)\n",
    "    html = res.read()\n",
    "    bsObj = BeautifulSoup(html)\n",
    "    #finds only the links with class \"mw-anonuserlink\" which has IP addresses \n",
    "    #instead of usernames\n",
    "    ipAddresses = bsObj.findAll(\"a\", {\"class\":\"mw-anonuserlink\"})\n",
    "    addressList = set()\n",
    "    for ipAddress in ipAddresses:\n",
    "        addressList.add(ipAddress.get_text())\n",
    "    return addressList\n",
    "\n",
    "\n",
    "def getCountry(ipAddress):\n",
    "    try:\n",
    "        response = urllib2.urlopen(\"http://freegeoip.net/json/\"\n",
    "                           +ipAddress).read().decode('utf-8')\n",
    "    except HTTPError:\n",
    "        return None\n",
    "    responseJson = json.loads(response)\n",
    "    return responseJson.get(\"country_code\")\n",
    "    \n",
    "links = getLinks(\"/wiki/Basel\")\n",
    "\n",
    "\n",
    "while(len(links) > 13):\n",
    "    for link in links:\n",
    "        print(\"-------------------\") \n",
    "        historyIPs = getHistoryIPs(link.attrs[\"href\"])\n",
    "        for historyIP in historyIPs:\n",
    "            country = getCountry(historyIP)\n",
    "            if country is not None:\n",
    "                print(historyIP+\" is from \"+country)\n",
    "\n",
    "    newLink = links[random.randint(0, len(links)-1)].attrs[\"href\"]\n",
    "    links = getLinks(newLink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## HTML Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        print(\"Encountered a start tag:\", tag)\n",
    "    def handle_endtag(self, tag):\n",
    "        print(\"Encountered an end tag :\", tag)\n",
    "    def handle_data(self, data):\n",
    "        print(\"Encountered some data  :\", data)\n",
    "\n",
    "parser = MyHTMLParser()\n",
    "parser.feed('<html><head><title>Test</title></head>'\n",
    "            '<body><h1>Parse me!</h1></body></html>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import urllib2\n",
    "\n",
    "\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    counter = 0 \n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if self.counter <10:\n",
    "            print(\"Encountered a start tag:\", tag)\n",
    "            self.counter +=1\n",
    "    def handle_endtag(self, tag):\n",
    "        if self.counter <10:\n",
    "            print(\"Encountered an end tag :\", tag)\n",
    "            self.counter +=1\n",
    "    def handle_data(self, data):\n",
    "        if self.counter <10:\n",
    "            print(\"Encountered some data  :\", data)\n",
    "            self.counter +=1\n",
    "\n",
    "res = urllib2.urlopen(\"http://www.nzz.ch\")        \n",
    "encoding = res.headers.getparam('charset')\n",
    "html = res.read().decode(encoding)             \n",
    "parser = MyHTMLParser()\n",
    "parser.feed(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import urllib2\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    \n",
    "    catch = 0\n",
    "    catchline = \"\"\n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        #print(\"Encountered a start tag:\", tag)\n",
    "        for attr in attrs:\n",
    "            #print(\"Attr\", attr)\n",
    "            #print(attr[1])\n",
    "            if attr[1] == \"title__catchline\":\n",
    "                self.catch = 2\n",
    "            elif attr[1] == \"title__name\":\n",
    "                self.catch = 1\n",
    "            else:\n",
    "                self.catch = 0\n",
    "            \n",
    "#    def handle_endtag(self, tag):\n",
    "        #print(\"Encountered an end tag :\", tag)\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if self.catch==1:\n",
    "            print self.catchline+\": \"+data\n",
    "            self.catchline = \"\"\n",
    "            \n",
    "        if self.catch==2:\n",
    "            self.catchline = data\n",
    "\n",
    "res = urllib2.urlopen(\"http://www.nzz.ch\")        \n",
    "encoding = res.headers.getparam('charset')\n",
    "html = res.read().decode(encoding)             \n",
    "parser = MyHTMLParser()\n",
    "parser.feed(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Rubriken NZZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import urllib2\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    \n",
    "    rubrik = 0\n",
    "    catch = 0\n",
    "    tmp = 0\n",
    "    \n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        #print(\"Encountered a start tag:\", tag)\n",
    "        for attr in attrs:\n",
    "            #print(\"Attr\", attr)\n",
    "            #print(attr[1])\n",
    "            if attr[1]!=None and attr[1].find(\"container__title \")!=-1 :\n",
    "                #print(\"Rubrik\")\n",
    "                self.rubrik += 1\n",
    "                self.catch = 1\n",
    "            else:\n",
    "                #self.catch = 0\n",
    "                self.tmp=0\n",
    "    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == \"a\" or tag == \"div\":\n",
    "            self.catch=0\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if self.catch==1:\n",
    "            if len(data) > 2:\n",
    "                print(str(self.rubrik)+\". \"+data)\n",
    "            \n",
    "\n",
    "res = urllib2.urlopen(\"http://www.nzz.ch\")        \n",
    "encoding = res.headers.getparam('charset')\n",
    "html = res.read().decode(encoding)             \n",
    "parser = MyHTMLParser()\n",
    "parser.feed(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Find Last Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import urllib2\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    \n",
    "    newesttime = 0\n",
    "    title = \"\"\n",
    "    tmp = 0\n",
    "    catch=0\n",
    "    \n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        #print(\"Encountered a start tag:\", tag)\n",
    "        if tag == \"div\":\n",
    "            for attr in attrs:\n",
    "                if attr[1] == \"title__name\":\n",
    "                    self.catch = 1\n",
    "            \n",
    "        \n",
    "        if tag == \"time\":\n",
    "            for attr in attrs:\n",
    "            #print(\"Attr\", attr)\n",
    "            #print(attr[1])\n",
    "                if attr[1]!=None and attr[0].find(\"datetime\")!=-1 :\n",
    "                    print attr[1]\n",
    "                \n",
    "\n",
    "    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == \"a\" or tag == \"div\":\n",
    "            self.catch=0\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if self.catch==1:\n",
    "            if len(data) > 2:\n",
    "                print(data)\n",
    "            \n",
    "\n",
    "res = urllib2.urlopen(\"http://www.nzz.ch\")        \n",
    "encoding = res.headers.getparam('charset')\n",
    "html = res.read().decode(encoding)             \n",
    "parser = MyHTMLParser()\n",
    "parser.feed(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAZ feature Story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import urllib2\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    \n",
    "    rubrik = 0\n",
    "    catch = 0\n",
    "    tmp = 0\n",
    "    \n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == \"div\":\n",
    "            for attr in attrs:\n",
    "                if attr[1]!=None and attr[1].find(\"featureStory\")!=-1 :\n",
    "                    self.rubrik += 1\n",
    "                    self.catch = 1\n",
    "                    #print(tag)\n",
    "                    break\n",
    "        if tag == \"a\" and self.catch ==1:\n",
    "            self.catch =2\n",
    "    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == \"a\":\n",
    "            self.catch=0\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if self.catch==2:\n",
    "            if len(data) > 1 and data != \"  \":\n",
    "                print(str(self.rubrik)+\". \"+data)\n",
    "            \n",
    "\n",
    "res = urllib2.urlopen(\"http://bazonline.ch/\")        \n",
    "encoding = res.headers.getparam('charset')\n",
    "html = res.read().decode(encoding)             \n",
    "parser = MyHTMLParser()\n",
    "parser.feed(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baz Rubriken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import urllib2\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    \n",
    "    rubrik = 0\n",
    "    catch = 0\n",
    "    tmp = 0\n",
    "    \n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == \"div\":\n",
    "            for attr in attrs:\n",
    "                if attr[1]!=None and attr[1].find(\"ressortGroup\")!=-1 :\n",
    "                    self.rubrik += 1\n",
    "                    self.catch = 1\n",
    "                    #print(tag)\n",
    "                    break\n",
    "        if tag == \"a\" and self.catch ==1:\n",
    "            self.catch =2\n",
    "    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == \"a\":\n",
    "            self.catch=0\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if self.catch==2:\n",
    "            if len(data) > 1 and data != \"  \":\n",
    "                print(str(self.rubrik)+\". \"+data)\n",
    "            \n",
    "\n",
    "res = urllib2.urlopen(\"http://bazonline.ch/\")        \n",
    "encoding = res.headers.getparam('charset')\n",
    "html = res.read().decode(encoding)             \n",
    "parser = MyHTMLParser()\n",
    "parser.feed(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Baz Artikelliste nach Zeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import urllib2\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    \n",
    "    rubrik = 0\n",
    "    catch = 0\n",
    "    tmp = 0\n",
    "    storyId = \"\"\n",
    "    title = \"\"\n",
    "    message = \"\"\n",
    "    info =[]\n",
    "    \n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == \"div\":\n",
    "            for attr in attrs:\n",
    "                if attr[1]!=None and attr[1].find(\"articleStory_\")!=-1 :\n",
    "                    self.storyId=attr[1]\n",
    "                    self.rubrik += 1\n",
    "                    self.info.append(self.rubrik)\n",
    "                    self.info.append(attr[1])\n",
    "                    self.catch = 1\n",
    "                    #print(tag)\n",
    "                    break\n",
    "                    \n",
    "        if tag == \"a\" and self.catch ==1:\n",
    "            self.catch = 2\n",
    "        \n",
    "        if tag ==\"h5\" and self.catch == 3:\n",
    "            self.catch = 4\n",
    "        if tag ==\"em\" and self.catch == 4:\n",
    "            self.catch = 5\n",
    "        if tag ==\"span\" and self.catch == 5:\n",
    "            self.catch = 7\n",
    "    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == \"a\":\n",
    "            self.catch=3\n",
    "            \n",
    "        if tag == \"em\" and self.catch == 5 or self.catch == 6:\n",
    "            self.catch=0\n",
    "            self.info=[]\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if self.catch == 2:\n",
    "            if len(data) > 1 and data != \"  \":\n",
    "                self.title=data\n",
    "                #self.info.append(self.storyId)\n",
    "                self.info.append(data)\n",
    "        elif self.catch == 5:\n",
    "            if data.find(\"\\t\\t\\t\\t\") == -1 and data != \" \":\n",
    "                self.message = str(self.rubrik)+\". \"+self.storyId+\":\"+self.title+\", \"+data\n",
    "                self.info.append(data)\n",
    "                print self.info\n",
    "        elif self.catch == 7:\n",
    "            self.info.append(data)\n",
    "            print data\n",
    "            print(\"catched\")\n",
    "            \n",
    "\n",
    "res = urllib2.urlopen(\"http://bazonline.ch/\")        \n",
    "encoding = res.headers.getparam('charset')\n",
    "html = res.read().decode(encoding)             \n",
    "parser = MyHTMLParser()\n",
    "parser.feed(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titel des ersten Artikel aus der Rubrik digital von 20 Minuten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import urllib2\n",
    "\n",
    "class DigitalHeadLine(HTMLParser):\n",
    "    \n",
    "    capture_txt=False\n",
    "    headlines =[]\n",
    "    \n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == \"h2\":\n",
    "            if \"data-vr-contentbox\" in dict(attrs):\n",
    "                self.capture_txt=True\n",
    "        return      \n",
    "        \n",
    "    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == \"h2\":\n",
    "             if self.capture_txt == True:\n",
    "                self.capture_txt=False\n",
    "        return\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if self.capture_txt == True:\n",
    "            self.headlines.append(data)\n",
    "        return\n",
    "    \n",
    "    def getheadlines(self):\n",
    "        print self.headlines[0]\n",
    "            \n",
    "            \n",
    "url=\"http://www.20min.ch/digital/\"\n",
    "res = urllib2.urlopen(url)        \n",
    "encoding = \"iso-8859-1\"\n",
    "html = res.read().decode(encoding)   \n",
    "parser = DigitalHeadLine()\n",
    "parser.feed(html)\n",
    "parser.getheadlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "attrs =[('blim','sim'),('wum','chum')]\n",
    "d = dict(attrs)\n",
    "print d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### GeoCoder Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://maps.googleapis.com/maps/api/geocode/json'\n",
    "params = {'sensor': 'false', 'address': 'Mountain View, CA'}\n",
    "r = requests.get(url, params=params)\n",
    "results = r.json()['results']\n",
    "location = results[0]['geometry']['location']\n",
    "location['lat'], location['lng']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import pprint\n",
    "import json\n",
    "add = \"Universitätstrasse 6 ETH Zürich\"\n",
    "add = urllib2.quote(add)\n",
    "geocode_url = \"http://maps.googleapis.com/maps/api/geocode/json?address=%s&sensor=false&region=uk\" % add\n",
    "print geocode_url\n",
    "req = urllib2.urlopen(geocode_url)\n",
    "jsonResponse = json.loads(req.read())\n",
    "pprint.pprint(jsonResponse) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import pprint\n",
    "import json\n",
    "add = \"Universitätstrasse 6 ETH Zürich\"\n",
    "add = urllib2.quote(add)\n",
    "print add\n",
    "geocode_url = \"http://maps.googleapis.com/maps/api/geocode/json?address=%s&sensor=false\" % add\n",
    "print geocode_url\n",
    "req = urllib2.urlopen(geocode_url)\n",
    "jsonResponse = json.loads(req.read())\n",
    "pprint.pprint(jsonResponse) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weg in den Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import pprint\n",
    "import json\n",
    "import re\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "\n",
    "start = \"Universitätstrasse 6 ETH Zürich\"\n",
    "ziel = \"Zoo Zürich\"\n",
    "sprache = \"de\" # \"en\"\n",
    "art = \"walking\" #driving walking transit bicycling\n",
    "#start = urllib2.quote(start.encode('utf-8'))\n",
    "#ziel = urllib2.quote(ziel.encode('utf-8'))\n",
    "start = urllib2.quote(start)\n",
    "ziel = urllib2.quote(ziel)\n",
    "geocode_url = \"https://maps.googleapis.com/maps/api/directions/json?origin=%s&destination=%s&avoid=highways&mode=%s&language=%s\" % (start,ziel,art,sprache)\n",
    "req = urllib2.urlopen(geocode_url)\n",
    "jsonResponse = json.loads(req.read())\n",
    "\n",
    "print \"Distanz:\" + jsonResponse['routes'][0]['legs'][0]['distance']['text']\n",
    "print \"Dauer:\" + jsonResponse['routes'][0]['legs'][0]['duration']['text']\n",
    "\n",
    "for step in jsonResponse['routes'][0]['legs'][0]['steps']:\n",
    "    print step['distance']['text'] + \" > \"+ remove_tags(step['html_instructions'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wolfram test\n",
    "\n",
    "http://api.wolframalpha.com/v2/query?appid=xxx&input=weather%20tomorrow%20in%20zurich&format=html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Kommentarstruktur eines Zeitungsartikels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=\"mist wurst\"\n",
    "\"mist\" in s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.20min.ch/community/storydiscussion/messageoverview.tmpl?storyid=22308172\n",
      "[{'author': 'Christian',\n",
      "  'content': ' das Ferien machen in Tunesien und allen Arabischen Staaten vermeiden sondern den Handel mit den Staaten ganz zum erliegen bringen. Einerseits Hassen sie die Christlichen Staaten machen aber auf der anderen Seite die hole Hand und appellieren an unsere Christlichen Werte. Sie bitten um Asyl oder wollen unser Schmutziges von Schweinefleisch Fressern Geld. Stellen wir den Handel ein wird es unserer Umwelt besser gehen wir denken mehr auf \\xc3\\x96kologisch und was durchaus machbar ist. K\\xc3\\xb6nnten auch unsere L\\xc3\\xa4nder friedlicher gestalten und Leben. Die Christen werden nach strich und Faden \\xc3\\xbcber den Tisch gez ',\n",
      "  'info': {'data-votedown': '33',\n",
      "           'data-voteup': '106',\n",
      "           'msg': '24',\n",
      "           'thread': '24'},\n",
      "  'time': 'am 30.08.2015 18:37',\n",
      "  'title': 'man sollte nicht nur'},\n",
      " {'author': 'Alexander Schneider',\n",
      "  'content': \" da jammern die Schweizer immer es sei alles zu teuer und man kommt kaum durch den Alltag. Aber f\\xc3\\xbcr Ferien haben sie dann Geld. Oder sind das auch geiz ist geil ferien Destinationen? ich glaub uns geht's gut,  aber im jammern waren wir schon immer Weltmeister. \",\n",
      "  'info': {'data-votedown': '41',\n",
      "           'data-voteup': '34',\n",
      "           'msg': '22',\n",
      "           'thread': '22'},\n",
      "  'time': 'am 30.08.2015 17:16',\n",
      "  'title': 'komisch'},\n",
      " {'author': 'Klarer Stect',\n",
      "  'content': ' ich finde es eher eine Schande, dass leute in L\\xc3\\xa4nder reisen (und diese somit unterst\\xc3\\xbctzen), welche gegen menschenrechte verstossen. Das sind die selben Schafe, die auch an die WM in Brasilien oder die olymp. spiele nach Russland gingen und sich vorhin dar\\xc3\\xbcber beschwerten, wie ungerecht das doch alles ist. dabei w\\xc3\\xa4re der Tourismus ein perfektes druckmittel, denn ohne Touris w\\xc3\\xa4ren diese L\\xc3\\xa4nder aufgeschmissen. Aber nein, Geld vor gerechtigkeit und nach mir die Sinthflut. ',\n",
      "  'info': {'data-votedown': '15',\n",
      "           'data-voteup': '29',\n",
      "           'msg': '20',\n",
      "           'thread': '20'},\n",
      "  'time': 'am 30.08.2015 17:14',\n",
      "  'title': 'Klartext:'},\n",
      " {'author': 'franz',\n",
      "  'content': ' das ist doch die \"Geiz ist geil\" Mentalit\\xc3\\xa4t und die Einstellung \"mir passiert eh nichts\". Schliesslich sind gef\\xc3\\xa4hrdete Destinationen bei den Buchungen g\\xc3\\xbcnstiger. Wenn dann was passiert, dann ist das Geschrei um so gr\\xc3\\xb6sser. ',\n",
      "  'info': {'data-votedown': '7',\n",
      "           'data-voteup': '29',\n",
      "           'msg': '18',\n",
      "           'thread': '18'},\n",
      "  'time': 'am 30.08.2015 16:53',\n",
      "  'title': 'Geiz ist geil'},\n",
      " {'author': 'Urs',\n",
      "  'content': ' Wieso sollte man jetzt wegen einem einmaligen Geschehnis, vermeiden in das jeweilige Land zu reisen? Die IS will genau das erreichen, dass zb. nach dem Terroranschlag niemand mehr nach Tunesien kommt und die Wirtschaft stark in der Krise ist. ',\n",
      "  'info': {'data-votedown': '0',\n",
      "           'data-voteup': '6',\n",
      "           'msg': '36',\n",
      "           'thread': '18'},\n",
      "  'reply': [{'author': 'AD',\n",
      "             'content': ' So ein Quatsch! Wenn ich letztes Jahr die T\\xc3\\xbcrkei gebucht habe und jetzt \\xc3\\xbcber 1000.- zum stornieren zahlen m\\xc3\\xbcsste, hat das nichts mit Geiz ist geil zu tun.  ',\n",
      "             'info': {},\n",
      "             'time': 'am 30.08.2015 18:44',\n",
      "             'title': 'Quatsch'},\n",
      "            {'author': 'Urs',\n",
      "             'content': ' Wieso sollte man jetzt wegen einem einmaligen Geschehnis, vermeiden in das jeweilige Land zu reisen? Die IS will genau das erreichen, dass zb. nach dem Terroranschlag niemand mehr nach Tunesien kommt und die Wirtschaft stark in der Krise ist. ',\n",
      "             'info': {},\n",
      "             'time': 'am 30.08.2015 21:35',\n",
      "             'title': 'Awa'}],\n",
      "  'time': 'am 30.08.2015 21:35',\n",
      "  'title': 'Awa'},\n",
      " {'author': 'Toni',\n",
      "  'content': ' Ich mache seid 4 Jahren keinen Urlaub mehr, nicht weil ich das Geld nicht habe, eher weil es Unsinn ist... Ich verbringe meine Ferien zuHause. ',\n",
      "  'info': {'data-votedown': '29',\n",
      "           'data-voteup': '24',\n",
      "           'msg': '16',\n",
      "           'thread': '16'},\n",
      "  'time': 'am 30.08.2015 16:19',\n",
      "  'title': 'Der '},\n",
      " {'author': 'Mike Meier',\n",
      "  'content': ' Da der Schweizer Franken stark, noch gen\\xc3\\xbcgend Geld vorhanden und Ferien machen in der Schweiz selbst f\\xc3\\xbcr Einheimische teuer ist, geht man lieber ins Ausland. ',\n",
      "  'info': {'data-votedown': '21',\n",
      "           'data-voteup': '75',\n",
      "           'msg': '10',\n",
      "           'thread': '10'},\n",
      "  'time': 'am 30.08.2015 15:15',\n",
      "  'title': 'Die Gunst der Stunde'},\n",
      " {'author': 'Kurt Zamperl',\n",
      "  'content': ' Ich empfehle grunds\\xc3\\xa4tzlich nicht in L\\xc3\\xa4nder zu reisen wo man als Tourist nur als wandelnde Geldquelle auf zwei Beinen gesehen wird. Neu wird man jetzt aber auch noch als Zielscheibe gesehen, somit hat man nun schon zwei sehr gute Gr\\xc3\\xbcnde diesen L\\xc3\\xa4ndern fernzubleiben. Unterdessen reisen ja mehr Tunesier in die Schweiz als Schweizer nach Tunesien. ',\n",
      "  'info': {'data-votedown': '25',\n",
      "           'data-voteup': '41',\n",
      "           'msg': '8',\n",
      "           'thread': '8'},\n",
      "  'time': 'am 30.08.2015 14:22',\n",
      "  'title': 'Nicht in Abzock-L\\xc3\\xa4nder reisen'},\n",
      " {'author': 'Fredy Himbi',\n",
      "  'content': ' Als wandelnde Geldquelle kann ich mir Ferien in der Schweiz kaum mehr leisten. Darum gehe ich ins nahe Ausland wo ich als wandelnde Geldquelle immer noch g\\xc3\\xbcnstiger davon komme als hier. Auch wenn ich mein Geld in der Schweiz verdiene. ',\n",
      "  'info': {'data-votedown': '12',\n",
      "           'data-voteup': '28',\n",
      "           'msg': '12',\n",
      "           'thread': '8'},\n",
      "  'reply': [{'author': 'Fredy Himbi',\n",
      "             'content': ' Als wandelnde Geldquelle kann ich mir Ferien in der Schweiz kaum mehr leisten. Darum gehe ich ins nahe Ausland wo ich als wandelnde Geldquelle immer noch g\\xc3\\xbcnstiger davon komme als hier. Auch wenn ich mein Geld in der Schweiz verdiene. ',\n",
      "             'info': {},\n",
      "             'time': 'am 30.08.2015 15:57',\n",
      "             'title': 'Wandelnde Geldquelle'}],\n",
      "  'time': 'am 30.08.2015 15:57',\n",
      "  'title': 'Wandelnde Geldquelle'},\n",
      " {'author': 'Brumm',\n",
      "  'content': ' Sonst k\\xc3\\xb6nnte man nicht mehr in die Ferien, nicht mehr essen, trinken, atmen etc.. ',\n",
      "  'info': {'data-votedown': '38',\n",
      "           'data-voteup': '127',\n",
      "           'msg': '4',\n",
      "           'thread': '4'},\n",
      "  'time': 'am 30.08.2015 14:16',\n",
      "  'title': 'Zum Gl\\xc3\\xbcck glauben nicht alle den Medien'},\n",
      " {'author': 'Ronny B',\n",
      "  'content': ' Ich war vor zwei Wochen auf Kos und es ist bei weitem nicht so heftig wie die Medien schreiben. In der Stadt hat es viele Fl\\xc3\\xbcchtlinge und sie schlafen draussen ja, aber ausserhalb merkt man rein gar nichts. Zudem sind diejenigen in der Stadt sehr friedlich und lassen Touristen in Ruhe. Also kein Grund sein Urlaub abzusagen.  ',\n",
      "  'info': {'data-votedown': '47',\n",
      "           'data-voteup': '61',\n",
      "           'msg': '2',\n",
      "           'thread': '2'},\n",
      "  'time': 'am 30.08.2015 13:37',\n",
      "  'title': 'Nicht so schlimm wie gesagt'}]\n"
     ]
    }
   ],
   "source": [
    "from urllib2 import urlopen\n",
    "import pprint\n",
    "from HTMLParser import HTMLParser\n",
    "\n",
    "def get_comment_par(attrs):\n",
    "    d = {}\n",
    "    for attr in attrs:\n",
    "        if attr[0] == 'id':\n",
    "            tmp = attr[1].split('_')\n",
    "            d['thread'] = tmp[0][6:]\n",
    "            d['msg'] = tmp[1][3:]\n",
    "        if attr[0] == \"data-voteup\" or attr[0] == \"data-votedown\":\n",
    "            d[attr[0]] = attr[1]\n",
    "    return d\n",
    "\n",
    "\n",
    "class LinksExtractor(HTMLParser): # derive new HTML parser\n",
    "\n",
    "    def __init__(self) :        # class constructor\n",
    "        HTMLParser.__init__(self)  # base class constructor\n",
    "        self.comments = []        # create an empty list for storing hyperlinks\n",
    "        self.insidecomment = False\n",
    "        self.intitle = False\n",
    "        self.inauthor = False\n",
    "        self.intime = False\n",
    "        self.incontent = False\n",
    "        self.inentry = False\n",
    "        self.inreplies = False\n",
    "        self.info = None\n",
    "        self.title = \"\"\n",
    "        self.content = \"\"\n",
    "        self.author =\"\"\n",
    "        self.time = \"\"\n",
    "        self.comment = None \n",
    "        self.replies = None\n",
    "        self.divlevel = 0\n",
    "        self.lilevel = 0\n",
    "        \n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'li':\n",
    "            self.lilevel += 1\n",
    "            for attr in attrs:\n",
    "                if attr[0] == 'class':\n",
    "                    if attr[1] == 'comment':\n",
    "                        self.insidecomment = True\n",
    "                        self.comment = {}\n",
    "                        self.info = get_comment_par(attrs)\n",
    "                        \n",
    "        elif tag == 'h3':\n",
    "            for attr in attrs:\n",
    "                if attr[0] == 'class':\n",
    "                    if attr[1] == 'title':\n",
    "                        self.intitle = True\n",
    "        elif tag == 'span':\n",
    "            for attr in attrs:\n",
    "                if attr[0] == 'class':\n",
    "                    if attr[1] == 'author':\n",
    "                        self.inauthor = True\n",
    "                    elif attr[1] == 'time': \n",
    "                        self.intime = True\n",
    "        elif tag == 'p':\n",
    "            for attr in attrs:\n",
    "                if attr[0] == 'class':\n",
    "                    if attr[1] == 'content':\n",
    "                        self.incontent = True\n",
    "        elif tag == 'div':\n",
    "            self.divlevel += 1\n",
    "            for attr in attrs:\n",
    "                if attr[0] == 'class':\n",
    "                    if attr[1] == 'entry':\n",
    "                        self.inentry = True\n",
    "                    elif 'replies' in attr[1]:\n",
    "                        self.replies =[]\n",
    "                        self.inreplies = True\n",
    "                        \n",
    "            \n",
    "            \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'li':\n",
    "            self.lilevel -= 1\n",
    "            if self.lilevel == 0:\n",
    "                self.insidecomment = False\n",
    "        elif tag == 'h3':\n",
    "            self.intitle = False\n",
    "        elif tag == 'span':\n",
    "            self.intime = False\n",
    "            self.inauthor = False\n",
    "        elif tag == 'p':\n",
    "            # collect all comment information\n",
    "            \n",
    "            if self.incontent:\n",
    "                if self.inentry:\n",
    "                    self.comment['info'] = self.info\n",
    "                    self.info = {}\n",
    "                    self.comment['author'] = self.author\n",
    "                    self.comment['time'] = self.time\n",
    "                    self.comment['title'] = self.title\n",
    "                    self.comment['content'] = self.content\n",
    "                    \n",
    "                if self.inreplies:\n",
    "                    col ={}\n",
    "                    col['info'] = self.info\n",
    "                    self.info = {}\n",
    "                    col['author'] = self.author\n",
    "                    col['time'] = self.time\n",
    "                    col['title'] = self.title\n",
    "                    col['content'] = self.content\n",
    "                    self.replies.append(col)\n",
    "                    \n",
    "            \n",
    "            self.incontent = False\n",
    "        elif tag == 'div':\n",
    "            self.divlevel -= 1\n",
    "            if self.divlevel == 0:\n",
    "                if self.inentry:\n",
    "                    self.comments.append(self.comment)\n",
    "                if self.inreplies:\n",
    "                    self.comment['reply'] = self.replies\n",
    "                self.inentry = False\n",
    "                self.inreplies = False\n",
    "        return\n",
    "        \n",
    "    def handle_data(self, data):\n",
    "        if self.intitle:\n",
    "            self.title = data\n",
    "        elif self.inauthor:\n",
    "            self.author = data\n",
    "        elif self.intime:\n",
    "            self.time = data\n",
    "        elif self.incontent:\n",
    "            self.content = data\n",
    "        return\n",
    "    \n",
    "    def get_comments(self) :     # return the list of extracted links\n",
    "        return self.comments\n",
    "\n",
    "\n",
    "\n",
    "zeitung = 'http://www.20min.ch';\n",
    "rubriken = ['schweiz'];\n",
    "\n",
    "storyid = '22308172'\n",
    "kommentare = zeitung + '/community/storydiscussion/messageoverview.tmpl?storyid=' + storyid\n",
    "print kommentare\n",
    "\n",
    "response = urlopen(kommentare,timeout=3)\n",
    "html = response.read()\n",
    "#pprint.pprint(html)\n",
    "\n",
    "\n",
    "p = LinksExtractor()\n",
    "p.feed(html) \n",
    "pprint.pprint(p.get_comments())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
