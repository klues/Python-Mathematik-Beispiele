{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib2 import urlopen\n",
    "endpoint = \"http://www.tigerjython.ch\"\n",
    "endpoint = \"http://python.org\"\n",
    "response = urlopen(endpoint)\n",
    "html = response.read()\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET HTML mit Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib2 import urlopen\n",
    "endpoint = \"http://www.tigerjython.ch/index.php?inhalt_links=navigation.inc.php&inhalt_mitte=robotik/lernendeRobot.inc.php\"\n",
    "endpoint = \"http://www.tigerjython.ch\"\n",
    "response = urlopen(endpoint)\n",
    "html = response.read()\n",
    "encodedhtml = unicode(html, 'iso-8859-1')\n",
    "#print(html)\n",
    "print(encodedhtml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET RESPONSE.Headers & HTTP STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib2 import urlopen\n",
    "\n",
    "conn = urlopen(\"http://www.tigerjython.ch\")\n",
    "status = conn.getcode()\n",
    "reason = conn.msg\n",
    "\n",
    "print(status,reason)\n",
    "print(con.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib2 import urlopen\n",
    "\n",
    "endpoint = \"http://www.tigerjython.ch\"\n",
    "endpoint = \"http://www.tigerjython.ch/index.php?inhalt_links=navigation.inc.php&inhalt_mitte=lernumgebung/lernumgebung.inc.php\"\n",
    "endpoint = \"http://pdf.tigerjython.ch\"\n",
    "\n",
    "handler = urlopen(endpoint)\n",
    "headers = handler.headers\n",
    "code = handler.getcode()\n",
    "msg = handler.msg\n",
    "handler.close()\n",
    "\n",
    "print(\"HTTP-Status:\",code)\n",
    "print(msg)\n",
    "print(headers)\n",
    "print handler.headers['content-type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## JSON Informationen über github User mgje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib2 import urlopen\n",
    "\n",
    "endpoint = \"https://api.github.com/users/mgje\"\n",
    "handler = urlopen(endpoint)\n",
    "\n",
    "j_obj = json.loads(handler.read())\n",
    "\n",
    "userid = j_obj['login']\n",
    "name = j_obj['name']\n",
    "created_at = j_obj['created_at']\n",
    "\n",
    "print('GitHub user ',userid)\n",
    "print('Name: ',name)\n",
    "print('since: ',created_at)\n",
    "\n",
    "handler.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TigerJython Beispiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2, json\n",
    "\n",
    "search = \"tigerjython+lernen\"\n",
    "url = \"http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\" + search\n",
    "responseStr = urllib2.urlopen(url).read()\n",
    "response = json.loads(responseStr)\n",
    "\n",
    "#print \"response:\\n\" + str(response) + \"\\n\"\n",
    "\n",
    "responseData = response[\"responseData\"]\n",
    "#print \"reponseData:\\n\" + str(responseData) + \"\\n\"\n",
    "\n",
    "results = responseData[\"results\"]\n",
    "#print \"results:\\n\" + str(results) + \"\\n\"\n",
    "\n",
    "for result in results:\n",
    "    title = result[\"title\"]\n",
    "    url = result[\"url\"]\n",
    "    print title + \" ---- \" + url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Parsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "from urllib2 import urlopen\n",
    "\n",
    "# create a subclass and override the handler methods\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        print \"Encountered a start tag:\", tag\n",
    "    def handle_endtag(self, tag):\n",
    "        print \"Encountered an end tag :\", tag\n",
    "    def handle_data(self, data):\n",
    "        print \"Encountered some data  :\", data\n",
    "\n",
    "\n",
    "endpoint = \"http://www.tigerjython.ch\"\n",
    "response = urlopen(endpoint)\n",
    "html = response.read()\n",
    "print(html)\n",
    "\n",
    "# instantiate the parser and fed it some HTML\n",
    "parser = MyHTMLParser()\n",
    "parser.feed(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more complex Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "from htmlentitydefs import name2codepoint\n",
    "from urllib2 import urlopen\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        print \"Start tag:\", tag\n",
    "        for attr in attrs:\n",
    "            print \"     attr:\", attr\n",
    "    def handle_endtag(self, tag):\n",
    "        print \"End tag  :\", tag\n",
    "    def handle_data(self, data):\n",
    "        print \"Data     :\", data\n",
    "    def handle_comment(self, data):\n",
    "        print \"Comment  :\", data\n",
    "    def handle_entityref(self, name):\n",
    "        c = unichr(name2codepoint[name])\n",
    "        print \"Named ent:\", c\n",
    "    def handle_charref(self, name):\n",
    "        if name.startswith('x'):\n",
    "            c = unichr(int(name[1:], 16))\n",
    "        else:\n",
    "            c = unichr(int(name))\n",
    "        print \"Num ent  :\", c\n",
    "    def handle_decl(self, data):\n",
    "        print \"Decl     :\", data\n",
    "\n",
    "        \n",
    "endpoint = \"http://www.tigerjython.ch\"\n",
    "response = urlopen(endpoint)\n",
    "html = response.read()\n",
    "print(html)\n",
    "\n",
    "parser = MyHTMLParser()\n",
    "parser.feed(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nach bestimmten Inhalten Parsen\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "\n",
    "\n",
    "class AllLanguages(HTMLParser):\n",
    "    def __init__(self):\n",
    "        HTMLParser.__init__(self)\n",
    "        self.inLink = False\n",
    "        self.dataArray = []\n",
    "        self.countLanguages = 0\n",
    "        self.lasttag = None\n",
    "        self.lastname = None\n",
    "        self.lastvalue = None\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        self.inLink = False\n",
    "        if tag == 'a':\n",
    "            for name, value in attrs:\n",
    "                if name == 'class' and value == 'Vocabulary':\n",
    "                    self.countLanguages += 1\n",
    "                    self.inLink = True\n",
    "                    self.lasttag = tag\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == \"a\":\n",
    "            self.inlink = False\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if self.lasttag == 'a' and self.inLink and data.strip():\n",
    "            print data\n",
    "\n",
    "\n",
    "parser = AllLanguages()\n",
    "parser.feed(\"\"\"\n",
    "<html>\n",
    "<head><title>Test</title></head>\n",
    "<body>\n",
    "<a href=\"http://wold.livingsources.org/vocabulary/1\" title=\"Swahili\" class=\"Vocabulary\">Swahili</a>\n",
    "<a href=\"http://wold.livingsources.org/contributor#schadebergthilo\" title=\"Thilo Schadeberg\" class=\"Contributor\">Thilo Schadeberg</a>\n",
    "<a href=\"http://wold.livingsources.org/vocabulary/2\" title=\"English\" class=\"Vocabulary\">English</a>\n",
    "<a href=\"http://wold.livingsources.org/vocabulary/2\" title=\"Russian\" class=\"Vocabulary\">Russian</a>\n",
    "</body>\n",
    "</html>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Parser Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dodo(v):\n",
    "    b = 33\n",
    "    if v == 2:\n",
    "        c = 4\n",
    "        d = 8\n",
    "    print vars()\n",
    "\n",
    "  \n",
    "dodo(1)\n",
    "dodo(2)\n",
    "\n",
    "attrs =[['href','www'],['blbla','dada']]\n",
    "\n",
    "tloc = map(lambda x: 1 if x[0]=='href' else 0,attrs)\n",
    "attr_loc = tloc.index(1)\n",
    "print tloc\n",
    "print attr_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "print bcolors.WARNING + \"Warning: No active frommets remain. Continue?\" + bcolors.ENDC\n",
    "\n",
    "CSI=\"\\x1B[\"\n",
    "\n",
    "print CSI+\"30;45m\" + \"Colored Text\" + CSI + \"0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "from urllib2 import urlopen\n",
    "\n",
    "class MyParse(HTMLParser):\n",
    "    def __init__(self):\n",
    "        #super() does not work for this class\n",
    "        HTMLParser.__init__(self)\n",
    "        self.tag_stack = []\n",
    "        self.attr_stack = []\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        #take the tag off the stack if it matches the next close tag\n",
    "        #if you are expecting unmatched tags, then this needs to be more robust\n",
    "        if self.tag_stack[len(self.tag_stack)-1][0] == tag:\n",
    "            self.tag_stack.pop()\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        #'data' is the text between tags, not necessarily\n",
    "        #matching tags\n",
    "        #this gives you a link to the last tag\n",
    "        if len(self.tag_stack) > 0:\n",
    "            tstack = self.tag_stack[len(self.tag_stack)-1]\n",
    "        else:\n",
    "            tstack = 0\n",
    "            \n",
    "        print(tstack,data)\n",
    "        #do something with the text\n",
    "            \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        #add tag to the stack\n",
    "        self.tag_stack.append([tag, attrs])\n",
    "        #if this tag is a link\n",
    "        if tag ==\"a\":\n",
    "            #these next few lines find if there is a hyperlink in the tag\n",
    "            tloc = map(lambda x: 1 if x[0]=='href' else 0,attrs)\n",
    "            try:\n",
    "                #did we find any hyperlinks\n",
    "                attr_loc = tloc.index(1)\n",
    "            except:\n",
    "                pass\n",
    "            # attr_loc only exists if we found a hyperlink\n",
    "            if vars().has_key('attr_loc'):\n",
    "                #append to the last item in the stack the location of the hyperlink\n",
    "                #note, this does not increase the length of the stack\n",
    "                #as we are putting it inside the last item on the stack\n",
    "                self.tag_stack[len(self.tag_stack)-1].append(attr_loc)\n",
    "\n",
    "endpoint = \"http://curioussystem.com\"\n",
    "response = urlopen(endpoint)\n",
    "html = response.read()\n",
    "#print(html)\n",
    "                \n",
    "p = MyParse()\n",
    "p.feed(html)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "from urllib2 import urlopen\n",
    "\n",
    "\n",
    "class LinksExtractor(HTMLParser): # derive new HTML parser\n",
    "\n",
    "    def __init__(self) :        # class constructor\n",
    "        HTMLParser.__init__(self)  # base class constructor\n",
    "        self.links = []        # create an empty list for storing hyperlinks\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag != 'a':\n",
    "            return\n",
    "        else:\n",
    "            if len(attrs) > 0 :\n",
    "                for attr in attrs :\n",
    "                    if attr[0] == \"href\" :         # ignore all non HREF attributes\n",
    "                        self.links.append(attr[1])\n",
    "            \n",
    "        \n",
    "    def get_links(self) :     # return the list of extracted links\n",
    "        return self.links\n",
    "    \n",
    "\n",
    "htmlparser = LinksExtractor() \n",
    "\n",
    "endpoint = \"http://www.tigerjython.ch/index.php?inhalt_links=navigation.inc.php&inhalt_mitte=internet/search.inc.php\"\n",
    "\n",
    "links = htmlparser.get_links()   # get the hyperlinks list\n",
    "\n",
    "#Print only external Links\n",
    "for link in links:\n",
    "    s = link[:7]\n",
    "    if s.lower() == 'http://':\n",
    "        print link + \"\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple httplib request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from httplib import HTTPConnection\n",
    "conn = HTTPConnection(\"www.python.org\")\n",
    "conn.request(\"GET\",\"/\")\n",
    "res = conn.getresponse()\n",
    "print res.status, res.reason\n",
    "conn.close()\n",
    "\"\"\"\n",
    "\n",
    "from httplib import HTTPConnectionb\n",
    "conn = HTTPConnection(\"www.tigerjython.ch\")\n",
    "conn.request(\"GET\",\"/index.html\")\n",
    "res = conn.getresponse()\n",
    "print res.status, res.reason\n",
    "for header in res.getheaders():\n",
    "    print(header[0]+\" : \"+header[1])\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if externallink works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "from urllib2 import urlopen\n",
    "from httplib import HTTPConnection, HTTPException\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "class LinksExtractor(HTMLParser): # derive new HTML parser\n",
    "\n",
    "    def __init__(self) :        # class constructor\n",
    "        HTMLParser.__init__(self)  # base class constructor\n",
    "        self.links = []        # create an empty list for storing hyperlinks\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag != 'a':\n",
    "            return\n",
    "        else:\n",
    "            if len(attrs) > 0 :\n",
    "                for attr in attrs :\n",
    "                    if attr[0] == \"href\" :         # ignore all non HREF attributes\n",
    "                        self.links.append(attr[1])\n",
    "            \n",
    "        \n",
    "    def get_links(self) :     # return the list of extracted links\n",
    "        return self.links\n",
    "    \n",
    "\n",
    "htmlparser = LinksExtractor() \n",
    "\n",
    "endpoint = \"http://www.tigerjython.ch/index.php?inhalt_links=navigation.inc.php&inhalt_mitte=anhang/links.inc.php\"  \n",
    "#endpoint = \"http://www.tigerjython.ch/index.php?inhalt_links=navigation.inc.php&inhalt_mitte=internet/search.inc.php\"\n",
    "\n",
    "response = urlopen(endpoint)\n",
    "html = response.read()\n",
    "response.close()\n",
    "\n",
    "encodedhtml=unicode(html, 'iso-8859-1')\n",
    "htmlparser.feed(encodedhtml)      # parse the file saving the info about links\n",
    "htmlparser.close()\n",
    "links = htmlparser.get_links()   # get the hyperlinks list\n",
    "\n",
    "# Color String\n",
    "CSI=\"\\x1B[\"\n",
    "\n",
    "#links=[\"http://www.unibas.ch\",\"http://www.blick.ch\",\"http://www.basel.ch\"]\n",
    "\n",
    "for link in links:\n",
    "    s = link[:7]\n",
    "    if s.lower() == 'http://':\n",
    "        url = link[7:]\n",
    "        if len(url.split('/'))>1:\n",
    "            site,path = url.split('/',1)\n",
    "            path = '/'+path\n",
    "        else:\n",
    "            url2 = url.split('/',1)\n",
    "            site = url2[0]\n",
    "            path = \"/\"\n",
    "            \n",
    "        conn = HTTPConnection(site,timeout=3)\n",
    "        try:\n",
    "            conn.request(\"HEAD\",path)\n",
    "            res = conn.getresponse()\n",
    "            conn.close()\n",
    "            if res.status != 200:\n",
    "                print CSI+\"30;45m\" + res.reason + \" \" + link +  CSI + \"0m\"+\"\\n\"\n",
    "            else:\n",
    "                print CSI+\"30;42m\" +\"OK: \"+CSI+\"30;42m\"+ link + CSI + \"0m\"+\"\\n\"\n",
    "        except:\n",
    "            print  CSI+\"30;45m\" + \"could not connect \" + link +   CSI + \"0m\"+\"\\n\"\n",
    "        \n",
    "        sys.stdout.flush()\n",
    "                \n",
    "\"END\"\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check URLS with urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "from urllib2 import urlopen\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "class LinksExtractor(HTMLParser): # derive new HTML parser\n",
    "\n",
    "    def __init__(self) :        # class constructor\n",
    "        HTMLParser.__init__(self)  # base class constructor\n",
    "        self.links = []        # create an empty list for storing hyperlinks\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag != 'a':\n",
    "            return\n",
    "        else:\n",
    "            if len(attrs) > 0 :\n",
    "                for attr in attrs :\n",
    "                    if attr[0] == \"href\" :         # ignore all non HREF attributes\n",
    "                        self.links.append(attr[1])\n",
    "            \n",
    "        \n",
    "    def get_links(self) :     # return the list of extracted links\n",
    "        return self.links\n",
    "    \n",
    "\n",
    "htmlparser = LinksExtractor() \n",
    "\n",
    "endpoint = \"http://www.tigerjython.ch/index.php?inhalt_links=navigation.inc.php&inhalt_mitte=anhang/links.inc.php\"  \n",
    "#endpoint = \"http://www.tigerjython.ch/index.php?inhalt_links=navigation.inc.php&inhalt_mitte=internet/search.inc.php\"\n",
    "\n",
    "response = urlopen(endpoint)\n",
    "html = response.read()\n",
    "response.close()\n",
    "\n",
    "encodedhtml=unicode(html, 'iso-8859-1')\n",
    "htmlparser.feed(encodedhtml)      # parse the file saving the info about links\n",
    "htmlparser.close()\n",
    "links = htmlparser.get_links()   # get the hyperlinks list\n",
    "\n",
    "# Color String\n",
    "CSI=\"\\x1B[\"\n",
    "\n",
    "#links=[\"http://www.unibas.ch\",\"http://www.blick.ch\",\"http://www.basel.ch\"]\n",
    "\n",
    "for link in links:\n",
    "    s = link[:7]\n",
    "    if s.lower() == 'http://':\n",
    "        try:\n",
    "            conn = urlopen(link,timeout=3)\n",
    "            code = conn.getcode()\n",
    "            msg = conn.msg\n",
    "            conn.close()\n",
    "            if code != 200:\n",
    "                print CSI+\"30;45m\" + msg + \" \" + link +  CSI + \"0m\"+\"\\n\"\n",
    "            else:\n",
    "                print CSI+\"30;42m\" +\"OK: \"+CSI+\"30;42m\"+ link + CSI + \"0m\"+\"\\n\"\n",
    "        except:\n",
    "            print  CSI+\"30;45m\" + \"could not connect \" + link +   CSI + \"0m\"+\"\\n\"\n",
    "        \n",
    "        sys.stdout.flush()\n",
    "                \n",
    "\"END\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2, json\n",
    "\n",
    "search = input(\"Enter a search string(AND-connect with +):\")\n",
    "url = \"http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\" + search\n",
    "responseStr = urllib2.urlopen(url).read()\n",
    "response = json.loads(responseStr)\n",
    "\n",
    "#print \"response:\\n\" + str(response) + \"\\n\"\n",
    "\n",
    "responseData = response[\"responseData\"]\n",
    "#print \"reponseData:\\n\" + str(responseData) + \"\\n\"\n",
    "\n",
    "results = responseData[\"results\"]\n",
    "#print \"results:\\n\" + str(results) + \"\\n\"\n",
    "\n",
    "for result in results:\n",
    "    title = result[\"title\"]\n",
    "    url = result[\"url\"]\n",
    "    print title + \" ---- \" + url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"Web Crawler/Spider\n",
    "\n",
    "This module implements a web crawler. This is very _basic_ only\n",
    "and needs to be extended to do anything usefull with the\n",
    "traversed pages.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import urllib2\n",
    "import urlparse\n",
    "import optparse\n",
    "from cgi import escape\n",
    "from traceback import format_exc\n",
    "from Queue import Queue, Empty as QueueEmpty\n",
    "\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "\n",
    "__version__ = \"0.2\"\n",
    "__copyright__ = \"CopyRight (C) 2008-2011 by James Mills\"\n",
    "__license__ = \"MIT\"\n",
    "__author__ = \"James Mills\"\n",
    "__author_email__ = \"James Mills, James dot Mills st dotred dot com dot au\"\n",
    "\n",
    "USAGE = \"%prog [options] <url>\"\n",
    "VERSION = \"%prog v\" + __version__\n",
    "\n",
    "AGENT = \"%s/%s\" % (__name__, __version__)\n",
    "\n",
    "class Crawler(object):\n",
    "\n",
    "    def __init__(self, root, depth, locked=True):\n",
    "        self.root = root\n",
    "        self.depth = depth\n",
    "        self.locked = locked\n",
    "        self.host = urlparse.urlparse(root)[1]\n",
    "        self.urls = []\n",
    "        self.links = 0\n",
    "        self.followed = 0\n",
    "\n",
    "    def crawl(self):\n",
    "        page = Fetcher(self.root)\n",
    "        page.fetch()\n",
    "        q = Queue()\n",
    "        for url in page.urls:\n",
    "            q.put(url)\n",
    "        followed = [self.root]\n",
    "\n",
    "        n = 0\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                url = q.get()\n",
    "            except QueueEmpty:\n",
    "                break\n",
    "\n",
    "            n += 1\n",
    "\n",
    "            if url not in followed:\n",
    "                try:\n",
    "                    host = urlparse.urlparse(url)[1]\n",
    "                    if self.locked and re.match(\".*%s\" % self.host, host):\n",
    "                        followed.append(url)\n",
    "                        self.followed += 1\n",
    "                        page = Fetcher(url)\n",
    "                        page.fetch()\n",
    "                        for i, url in enumerate(page):\n",
    "                            if url not in self.urls:\n",
    "                                self.links += 1\n",
    "                                q.put(url)\n",
    "                                self.urls.append(url)\n",
    "                        if n > self.depth and self.depth > 0:\n",
    "                            break\n",
    "                except Exception, e:\n",
    "                    print \"ERROR: Can't process url '%s' (%s)\" % (url, e)\n",
    "                    print format_exc()\n",
    "\n",
    "class Fetcher(object):\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.urls = []\n",
    "\n",
    "    def __getitem__(self, x):\n",
    "        return self.urls[x]\n",
    "\n",
    "    def _addHeaders(self, request):\n",
    "        request.add_header(\"User-Agent\", AGENT)\n",
    "\n",
    "    def open(self):\n",
    "        url = self.url\n",
    "        try:\n",
    "            request = urllib2.Request(url)\n",
    "            handle = urllib2.build_opener()\n",
    "        except IOError:\n",
    "            return None\n",
    "        return (request, handle)\n",
    "\n",
    "    def fetch(self):\n",
    "        request, handle = self.open()\n",
    "        self._addHeaders(request)\n",
    "        if handle:\n",
    "            try:\n",
    "                content = unicode(handle.open(request).read(), \"utf-8\",\n",
    "                        errors=\"replace\")\n",
    "                soup = BeautifulSoup(content)\n",
    "                tags = soup('a')\n",
    "            except urllib2.HTTPError, error:\n",
    "                if error.code == 404:\n",
    "                    print >> sys.stderr, \"ERROR: %s -> %s\" % (error, error.url)\n",
    "                else:\n",
    "                    print >> sys.stderr, \"ERROR: %s\" % error\n",
    "                tags = []\n",
    "            except urllib2.URLError, error:\n",
    "                print >> sys.stderr, \"ERROR: %s\" % error\n",
    "                tags = []\n",
    "            for tag in tags:\n",
    "                href = tag.get(\"href\")\n",
    "                if href is not None:\n",
    "                    url = urlparse.urljoin(self.url, escape(href))\n",
    "                    if url not in self:\n",
    "                        self.urls.append(url)\n",
    "\n",
    "def getLinks(url):\n",
    "    page = Fetcher(url)\n",
    "    page.fetch()\n",
    "    for i, url in enumerate(page):\n",
    "        print \"%d. %s\" % (i, url)\n",
    "\n",
    "def parse_options():\n",
    "    \"\"\"parse_options() -> opts, args\n",
    "\n",
    "    Parse any command-line options given returning both\n",
    "    the parsed options and arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    parser = optparse.OptionParser(usage=USAGE, version=VERSION)\n",
    "\n",
    "    parser.add_option(\"-q\", \"--quiet\",\n",
    "            action=\"store_true\", default=False, dest=\"quiet\",\n",
    "            help=\"Enable quiet mode\")\n",
    "\n",
    "    parser.add_option(\"-l\", \"--links\",\n",
    "            action=\"store_true\", default=False, dest=\"links\",\n",
    "            help=\"Get links for specified url only\")\n",
    "\n",
    "    parser.add_option(\"-d\", \"--depth\",\n",
    "            action=\"store\", type=\"int\", default=30, dest=\"depth\",\n",
    "            help=\"Maximum depth to traverse\")\n",
    "\n",
    "    opts, args = parser.parse_args()\n",
    "\n",
    "    if len(args) < 1:\n",
    "        parser.print_help()\n",
    "        raise SystemExit, 1\n",
    "\n",
    "    return opts, args\n",
    "\n",
    "def main():\n",
    "    opts, args = parse_options()\n",
    "\n",
    "    url = args[0]\n",
    "\n",
    "    if opts.links:\n",
    "        getLinks(url)\n",
    "        raise SystemExit, 0\n",
    "\n",
    "    depth = opts.depth\n",
    "\n",
    "    sTime = time.time()\n",
    "\n",
    "    print \"Crawling %s (Max Depth: %d)\" % (url, depth)\n",
    "    crawler = Crawler(url, depth)\n",
    "    crawler.crawl()\n",
    "    print \"\\n\".join(crawler.urls)\n",
    "\n",
    "    eTime = time.time()\n",
    "    tTime = eTime - sTime\n",
    "\n",
    "    print \"Found:    %d\" % crawler.links\n",
    "    print \"Followed: %d\" % crawler.followed\n",
    "    print \"Stats:    (%d/s after %0.2fs)\" % (\n",
    "            int(math.ceil(float(crawler.links) / tTime)), tTime)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib2 import urlopen, URLError\n",
    "\n",
    "fdurl = urlopen(\"http://www.unibas.ch\",timeout=2)\n",
    "#realsock = fdurl.fp._sock.fp._sock* # we want to close the \"real\" socket later\n",
    "realsock = fdurl.fp\n",
    "#req = Request(url, header)\n",
    "print dir(realsock)\n",
    "print realsock.__doc__\n",
    "print realsock.closed\n",
    "realsock.close()\n",
    "print realsock.closed\n",
    "try:\n",
    "    fdurl = urllib2.urlopen(\"http://www.tigerjython.ch\",timeout=2)\n",
    "    print fdurl.read()\n",
    "except urllib2.URLError,e:\n",
    "    print \"urlopen exception\", e\n",
    "    realsock.close() \n",
    "    fdurl.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import Queue\n",
    "import urllib2\n",
    "\n",
    "# utility - spawn a thread to execute target for each args\n",
    "def run_parallel_in_threads(target, args_list):\n",
    "    result = Queue.Queue()\n",
    "    # wrapper to collect return value in a Queue\n",
    "    def task_wrapper(*args):\n",
    "        result.put(target(*args))\n",
    "    threads = [threading.Thread(target=task_wrapper, args=args) for args in args_list]\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    return result\n",
    "\n",
    "def dummy_task(n):\n",
    "    for i in xrange(n):\n",
    "        time.sleep(0.1)\n",
    "    return n\n",
    "\n",
    "# below is the application code\n",
    "urls = [\n",
    "    ('http://www.google.com/',),\n",
    "    ('http://www.lycos.com/',),\n",
    "    ('http://www.bing.com/',),\n",
    "    ('http://www.altavista.com/',),\n",
    "    ('http://achewood.com/',),\n",
    "]\n",
    "\n",
    "def fetch(url):\n",
    "    return urllib2.urlopen(url).read()\n",
    "\n",
    "q =  run_parallel_in_threads(fetch, urls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Parsen mit BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "\n",
    "response = urllib2.urlopen(\"http://www.pythonscraping.com/pages/page3.html\")\n",
    "html = response.read()\n",
    "bsObj = BeautifulSoup(html)\n",
    "\n",
    "for sibling in bsObj.find(\"table\",{\"id\":\"giftList\"}).tr.next_siblings:\n",
    "    print(sibling) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "images = bsObj.findAll(\"img\", {\"src\":re.compile(\"\\.\\.\\/img\\/gifts/img.*\\.jpg\")})\n",
    "for image in images: \n",
    "    print(image[\"src\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WikiPedia by Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import random\n",
    "import re\n",
    "\n",
    "random.seed(datetime.datetime.now())\n",
    "\n",
    "def getLinks(articleUrl):\n",
    "    res = urllib2.urlopen(\"http://de.wikipedia.org\"+articleUrl)\n",
    "    html = res.read()\n",
    "    bsObj = BeautifulSoup(html)\n",
    "    return bsObj.find(\"div\", {\"id\":\"bodyContent\"}).findAll(\"a\", href=re.compile(\"^(/wiki/)((?!:).)*$\"))\n",
    "\n",
    "links = getLinks(\"/wiki/Basel\")\n",
    "while len(links) > 15:\n",
    "    newArticle = links[random.randint(0, len(links)-1)].attrs[\"href\"]\n",
    "    print(newArticle)\n",
    "    links = getLinks(newArticle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib2\n",
    "\n",
    "\n",
    "def getCountry(ipAddress):\n",
    "    response = urllib2.urlopen(\"http://freegeoip.net/json/\"+ipAddress).read().decode('utf-8')\n",
    "    responseJson = json.loads(response)\n",
    "    return responseJson.get(\"country_code\")\n",
    "\n",
    "print(getCountry(\"50.78.253.58\"))\n",
    "print(getCountry(\"131.152.1.1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiki History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import random\n",
    "import re\n",
    "\n",
    "random.seed(datetime.datetime.now())\n",
    "def getLinks(articleUrl):\n",
    "    res = urllib2.urlopen(\"http://de.wikipedia.org\"+articleUrl)\n",
    "    html = res.read()\n",
    "    bsObj = BeautifulSoup(html)\n",
    "    return bsObj.find(\"div\", {\"id\":\"bodyContent\"}).findAll(\"a\", \n",
    "                     href=re.compile(\"^(/wiki/)((?!:).)*$\"))\n",
    "\n",
    "def getHistoryIPs(pageUrl):\n",
    "    #Format of revision history pages is: \n",
    "    #http://en.wikipedia.org/w/index.php?title=Title_in_URL&action=history\n",
    "    pageUrl = pageUrl.replace(\"/wiki/\", \"\")\n",
    "    historyUrl = \"http://de.wikipedia.org/w/index.php?title=\"+pageUrl+\"&action=history\"\n",
    "    print(\"history url is: \"+historyUrl)\n",
    "    res = urllib2.urlopen(historyUrl)\n",
    "    html = res.read()\n",
    "    bsObj = BeautifulSoup(html)\n",
    "    #finds only the links with class \"mw-anonuserlink\" which has IP addresses \n",
    "    #instead of usernames\n",
    "    ipAddresses = bsObj.findAll(\"a\", {\"class\":\"mw-anonuserlink\"})\n",
    "    addressList = set()\n",
    "    for ipAddress in ipAddresses:\n",
    "        addressList.add(ipAddress.get_text())\n",
    "    return addressList\n",
    "\n",
    "\n",
    "def getCountry(ipAddress):\n",
    "    try:\n",
    "        response = urllib2.urlopen(\"http://freegeoip.net/json/\"\n",
    "                           +ipAddress).read().decode('utf-8')\n",
    "    except HTTPError:\n",
    "        return None\n",
    "    responseJson = json.loads(response)\n",
    "    return responseJson.get(\"country_code\")\n",
    "    \n",
    "links = getLinks(\"/wiki/Basel\")\n",
    "\n",
    "\n",
    "while(len(links) > 13):\n",
    "    for link in links:\n",
    "        print(\"-------------------\") \n",
    "        historyIPs = getHistoryIPs(link.attrs[\"href\"])\n",
    "        for historyIP in historyIPs:\n",
    "            country = getCountry(historyIP)\n",
    "            if country is not None:\n",
    "                print(historyIP+\" is from \"+country)\n",
    "\n",
    "    newLink = links[random.randint(0, len(links)-1)].attrs[\"href\"]\n",
    "    links = getLinks(newLink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## HTML Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        print(\"Encountered a start tag:\", tag)\n",
    "    def handle_endtag(self, tag):\n",
    "        print(\"Encountered an end tag :\", tag)\n",
    "    def handle_data(self, data):\n",
    "        print(\"Encountered some data  :\", data)\n",
    "\n",
    "parser = MyHTMLParser()\n",
    "parser.feed('<html><head><title>Test</title></head>'\n",
    "            '<body><h1>Parse me!</h1></body></html>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import urllib2\n",
    "\n",
    "\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    counter = 0 \n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if self.counter <10:\n",
    "            print(\"Encountered a start tag:\", tag)\n",
    "            self.counter +=1\n",
    "    def handle_endtag(self, tag):\n",
    "        if self.counter <10:\n",
    "            print(\"Encountered an end tag :\", tag)\n",
    "            self.counter +=1\n",
    "    def handle_data(self, data):\n",
    "        if self.counter <10:\n",
    "            print(\"Encountered some data  :\", data)\n",
    "            self.counter +=1\n",
    "\n",
    "res = urllib2.urlopen(\"http://www.nzz.ch\")        \n",
    "encoding = res.headers.getparam('charset')\n",
    "html = res.read().decode(encoding)             \n",
    "parser = MyHTMLParser()\n",
    "parser.feed(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import urllib2\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    \n",
    "    catch = 0\n",
    "    catchline = \"\"\n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        #print(\"Encountered a start tag:\", tag)\n",
    "        for attr in attrs:\n",
    "            #print(\"Attr\", attr)\n",
    "            #print(attr[1])\n",
    "            if attr[1] == \"title__catchline\":\n",
    "                self.catch = 2\n",
    "            elif attr[1] == \"title__name\":\n",
    "                self.catch = 1\n",
    "            else:\n",
    "                self.catch = 0\n",
    "            \n",
    "#    def handle_endtag(self, tag):\n",
    "        #print(\"Encountered an end tag :\", tag)\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if self.catch==1:\n",
    "            print self.catchline+\": \"+data\n",
    "            self.catchline = \"\"\n",
    "            \n",
    "        if self.catch==2:\n",
    "            self.catchline = data\n",
    "\n",
    "res = urllib2.urlopen(\"http://www.nzz.ch\")        \n",
    "encoding = res.headers.getparam('charset')\n",
    "html = res.read().decode(encoding)             \n",
    "parser = MyHTMLParser()\n",
    "parser.feed(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Rubriken NZZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import urllib2\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    \n",
    "    rubrik = 0\n",
    "    catch = 0\n",
    "    tmp = 0\n",
    "    \n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        #print(\"Encountered a start tag:\", tag)\n",
    "        for attr in attrs:\n",
    "            #print(\"Attr\", attr)\n",
    "            #print(attr[1])\n",
    "            if attr[1]!=None and attr[1].find(\"container__title \")!=-1 :\n",
    "                #print(\"Rubrik\")\n",
    "                self.rubrik += 1\n",
    "                self.catch = 1\n",
    "            else:\n",
    "                #self.catch = 0\n",
    "                self.tmp=0\n",
    "    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == \"a\" or tag == \"div\":\n",
    "            self.catch=0\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if self.catch==1:\n",
    "            if len(data) > 2:\n",
    "                print(str(self.rubrik)+\". \"+data)\n",
    "            \n",
    "\n",
    "res = urllib2.urlopen(\"http://www.nzz.ch\")        \n",
    "encoding = res.headers.getparam('charset')\n",
    "html = res.read().decode(encoding)             \n",
    "parser = MyHTMLParser()\n",
    "parser.feed(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Find Last Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import urllib2\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    \n",
    "    newesttime = 0\n",
    "    title = \"\"\n",
    "    tmp = 0\n",
    "    catch=0\n",
    "    \n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        #print(\"Encountered a start tag:\", tag)\n",
    "        if tag == \"div\":\n",
    "            for attr in attrs:\n",
    "                if attr[1] == \"title__name\":\n",
    "                    self.catch = 1\n",
    "            \n",
    "        \n",
    "        if tag == \"time\":\n",
    "            for attr in attrs:\n",
    "            #print(\"Attr\", attr)\n",
    "            #print(attr[1])\n",
    "                if attr[1]!=None and attr[0].find(\"datetime\")!=-1 :\n",
    "                    print attr[1]\n",
    "                \n",
    "\n",
    "    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == \"a\" or tag == \"div\":\n",
    "            self.catch=0\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if self.catch==1:\n",
    "            if len(data) > 2:\n",
    "                print(data)\n",
    "            \n",
    "\n",
    "res = urllib2.urlopen(\"http://www.nzz.ch\")        \n",
    "encoding = res.headers.getparam('charset')\n",
    "html = res.read().decode(encoding)             \n",
    "parser = MyHTMLParser()\n",
    "parser.feed(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAZ feature Story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import urllib2\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    \n",
    "    rubrik = 0\n",
    "    catch = 0\n",
    "    tmp = 0\n",
    "    \n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == \"div\":\n",
    "            for attr in attrs:\n",
    "                if attr[1]!=None and attr[1].find(\"featureStory\")!=-1 :\n",
    "                    self.rubrik += 1\n",
    "                    self.catch = 1\n",
    "                    #print(tag)\n",
    "                    break\n",
    "        if tag == \"a\" and self.catch ==1:\n",
    "            self.catch =2\n",
    "    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == \"a\":\n",
    "            self.catch=0\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if self.catch==2:\n",
    "            if len(data) > 1 and data != \"  \":\n",
    "                print(str(self.rubrik)+\". \"+data)\n",
    "            \n",
    "\n",
    "res = urllib2.urlopen(\"http://bazonline.ch/\")        \n",
    "encoding = res.headers.getparam('charset')\n",
    "html = res.read().decode(encoding)             \n",
    "parser = MyHTMLParser()\n",
    "parser.feed(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baz Rubriken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import urllib2\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    \n",
    "    rubrik = 0\n",
    "    catch = 0\n",
    "    tmp = 0\n",
    "    \n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == \"div\":\n",
    "            for attr in attrs:\n",
    "                if attr[1]!=None and attr[1].find(\"ressortGroup\")!=-1 :\n",
    "                    self.rubrik += 1\n",
    "                    self.catch = 1\n",
    "                    #print(tag)\n",
    "                    break\n",
    "        if tag == \"a\" and self.catch ==1:\n",
    "            self.catch =2\n",
    "    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == \"a\":\n",
    "            self.catch=0\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if self.catch==2:\n",
    "            if len(data) > 1 and data != \"  \":\n",
    "                print(str(self.rubrik)+\". \"+data)\n",
    "            \n",
    "\n",
    "res = urllib2.urlopen(\"http://bazonline.ch/\")        \n",
    "encoding = res.headers.getparam('charset')\n",
    "html = res.read().decode(encoding)             \n",
    "parser = MyHTMLParser()\n",
    "parser.feed(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Baz Artikelliste nach Zeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import urllib2\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    \n",
    "    rubrik = 0\n",
    "    catch = 0\n",
    "    tmp = 0\n",
    "    storyId = \"\"\n",
    "    title = \"\"\n",
    "    message = \"\"\n",
    "    info =[]\n",
    "    \n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == \"div\":\n",
    "            for attr in attrs:\n",
    "                if attr[1]!=None and attr[1].find(\"articleStory_\")!=-1 :\n",
    "                    self.storyId=attr[1]\n",
    "                    self.rubrik += 1\n",
    "                    self.info.append(self.rubrik)\n",
    "                    self.info.append(attr[1])\n",
    "                    self.catch = 1\n",
    "                    #print(tag)\n",
    "                    break\n",
    "                    \n",
    "        if tag == \"a\" and self.catch ==1:\n",
    "            self.catch = 2\n",
    "        \n",
    "        if tag ==\"h5\" and self.catch == 3:\n",
    "            self.catch = 4\n",
    "        if tag ==\"em\" and self.catch == 4:\n",
    "            self.catch = 5\n",
    "        if tag ==\"span\" and self.catch == 5:\n",
    "            self.catch = 7\n",
    "    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == \"a\":\n",
    "            self.catch=3\n",
    "            \n",
    "        if tag == \"em\" and self.catch == 5 or self.catch == 6:\n",
    "            self.catch=0\n",
    "            self.info=[]\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if self.catch == 2:\n",
    "            if len(data) > 1 and data != \"  \":\n",
    "                self.title=data\n",
    "                #self.info.append(self.storyId)\n",
    "                self.info.append(data)\n",
    "        elif self.catch == 5:\n",
    "            if data.find(\"\\t\\t\\t\\t\") == -1 and data != \" \":\n",
    "                self.message = str(self.rubrik)+\". \"+self.storyId+\":\"+self.title+\", \"+data\n",
    "                self.info.append(data)\n",
    "                print self.info\n",
    "        elif self.catch == 7:\n",
    "            self.info.append(data)\n",
    "            print data\n",
    "            print(\"catched\")\n",
    "            \n",
    "\n",
    "res = urllib2.urlopen(\"http://bazonline.ch/\")        \n",
    "encoding = res.headers.getparam('charset')\n",
    "html = res.read().decode(encoding)             \n",
    "parser = MyHTMLParser()\n",
    "parser.feed(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titel des ersten Artikel aus der Rubrik digital von 20 Minuten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import urllib2\n",
    "\n",
    "class DigitalHeadLine(HTMLParser):\n",
    "    \n",
    "    capture_txt=False\n",
    "    headlines =[]\n",
    "    \n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == \"h2\":\n",
    "            if \"data-vr-contentbox\" in dict(attrs):\n",
    "                self.capture_txt=True\n",
    "        return      \n",
    "        \n",
    "    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == \"h2\":\n",
    "             if self.capture_txt == True:\n",
    "                self.capture_txt=False\n",
    "        return\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if self.capture_txt == True:\n",
    "            self.headlines.append(data)\n",
    "        return\n",
    "    \n",
    "    def getheadlines(self):\n",
    "        print self.headlines[0]\n",
    "            \n",
    "            \n",
    "url=\"http://www.20min.ch/digital/\"\n",
    "res = urllib2.urlopen(url)        \n",
    "encoding = \"iso-8859-1\"\n",
    "html = res.read().decode(encoding)   \n",
    "parser = DigitalHeadLine()\n",
    "parser.feed(html)\n",
    "parser.getheadlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "attrs =[('blim','sim'),('wum','chum')]\n",
    "d = dict(attrs)\n",
    "print d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### GeoCoder Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://maps.googleapis.com/maps/api/geocode/json'\n",
    "params = {'sensor': 'false', 'address': 'Mountain View, CA'}\n",
    "r = requests.get(url, params=params)\n",
    "results = r.json()['results']\n",
    "location = results[0]['geometry']['location']\n",
    "location['lat'], location['lng']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import pprint\n",
    "import json\n",
    "add = \"Universitätstrasse 6 ETH Zürich\"\n",
    "add = urllib2.quote(add)\n",
    "geocode_url = \"http://maps.googleapis.com/maps/api/geocode/json?address=%s&sensor=false&region=uk\" % add\n",
    "print geocode_url\n",
    "req = urllib2.urlopen(geocode_url)\n",
    "jsonResponse = json.loads(req.read())\n",
    "pprint.pprint(jsonResponse) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import pprint\n",
    "import json\n",
    "add = \"Universitätstrasse 6 ETH Zürich\"\n",
    "add = urllib2.quote(add)\n",
    "print add\n",
    "geocode_url = \"http://maps.googleapis.com/maps/api/geocode/json?address=%s&sensor=false\" % add\n",
    "print geocode_url\n",
    "req = urllib2.urlopen(geocode_url)\n",
    "jsonResponse = json.loads(req.read())\n",
    "pprint.pprint(jsonResponse) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weg in den Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import pprint\n",
    "import json\n",
    "import re\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "\n",
    "start = \"Universitätstrasse 6 ETH Zürich\"\n",
    "ziel = \"Zoo Zürich\"\n",
    "sprache = \"de\" # \"en\"\n",
    "art = \"walking\" #driving walking transit bicycling\n",
    "#start = urllib2.quote(start.encode('utf-8'))\n",
    "#ziel = urllib2.quote(ziel.encode('utf-8'))\n",
    "start = urllib2.quote(start)\n",
    "ziel = urllib2.quote(ziel)\n",
    "geocode_url = \"https://maps.googleapis.com/maps/api/directions/json?origin=%s&destination=%s&avoid=highways&mode=%s&language=%s\" % (start,ziel,art,sprache)\n",
    "req = urllib2.urlopen(geocode_url)\n",
    "jsonResponse = json.loads(req.read())\n",
    "\n",
    "print \"Distanz:\" + jsonResponse['routes'][0]['legs'][0]['distance']['text']\n",
    "print \"Dauer:\" + jsonResponse['routes'][0]['legs'][0]['duration']['text']\n",
    "\n",
    "for step in jsonResponse['routes'][0]['legs'][0]['steps']:\n",
    "    print step['distance']['text'] + \" > \"+ remove_tags(step['html_instructions'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wolfram test\n",
    "\n",
    "http://api.wolframalpha.com/v2/query?appid=xxx&input=weather%20tomorrow%20in%20zurich&format=html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Kommentarstruktur eines Zeitungsartikels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=\"mist wurst\"\n",
    "\"mist\" in s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.20min.ch/community/storydiscussion/messageoverview.tmpl?storyid=18530412\n",
      "collect comment, aut: shady\n",
      "add comment\n",
      "collect comment, aut: Grunz\n",
      "collect comment, aut: Bubbl\n",
      "save reply: autor: Bubbl\n",
      "add replies\n",
      "add comment\n",
      "collect comment, aut: Sabi\n",
      "collect comment, aut: Cara\n",
      "save reply: autor: Cara\n",
      "add replies\n",
      "add comment\n",
      "collect comment, aut: JW Goethe \n",
      "collect comment, aut: Der Boi\n",
      "save reply: autor: Der Boi\n",
      "add replies\n",
      "add comment\n",
      "[{'author': 'shady',\n",
      "  'content': ' Achja die Medien mal wieder. Der Film l\\xc3\\xa4uft nicht so gut wie geplant und man sieht Cara mal im Ausgang trinken. Ergo hat sie jetzt ein Alkoholproblem. ',\n",
      "  'info': {'data-votedown': '4',\n",
      "           'data-voteup': '83',\n",
      "           'msg': '8',\n",
      "           'thread': '8'},\n",
      "  'time': 'am 31.08.2015 01:14',\n",
      "  'title': 'So ein Quatsch'},\n",
      " {'author': 'Bubbl',\n",
      "  'content': ' Bin ganz deiner Meinung! ',\n",
      "  'info': {'data-votedown': '1',\n",
      "           'data-voteup': '26',\n",
      "           'msg': '22',\n",
      "           'thread': '6'},\n",
      "  'reply': [{'author': 'Bubbl',\n",
      "             'content': ' Bin ganz deiner Meinung! ',\n",
      "             'info': {},\n",
      "             'time': 'am 31.08.2015 06:32',\n",
      "             'title': 'Eben!'}],\n",
      "  'time': 'am 31.08.2015 06:32',\n",
      "  'title': 'Eben!'},\n",
      " {'author': 'Cara',\n",
      "  'content': ' Stimmt, die ist das! Immer wenn ich die Werbung gesehen habe, hab ich mir nur gedacht, irgendwoher kenn ich diesen halbgaren Promi:-) ',\n",
      "  'info': {'data-votedown': '2',\n",
      "           'data-voteup': '3',\n",
      "           'msg': '26',\n",
      "           'thread': '4'},\n",
      "  'reply': [{'author': 'Cara',\n",
      "             'content': ' Stimmt, die ist das! Immer wenn ich die Werbung gesehen habe, hab ich mir nur gedacht, irgendwoher kenn ich diesen halbgaren Promi:-) ',\n",
      "             'info': {},\n",
      "             'time': 'am 31.08.2015 07:43',\n",
      "             'title': 'Ahhhhhh!'}],\n",
      "  'time': 'am 31.08.2015 07:43',\n",
      "  'title': 'Ahhhhhh!'},\n",
      " {'author': 'Der Boi',\n",
      "  'content': ' Der 1 Boi thight die richtigen Rhymes am spitten. Fly wie 1 Eagle, Cara Delevigne wie 1 uboot..\\nSheesh, buurrr ',\n",
      "  'info': {'data-votedown': '7',\n",
      "           'data-voteup': '10',\n",
      "           'msg': '24',\n",
      "           'thread': '2'},\n",
      "  'reply': [{'author': 'Der Boi',\n",
      "             'content': ' Der 1 Boi thight die richtigen Rhymes am spitten. Fly wie 1 Eagle, Cara Delevigne wie 1 uboot..\\nSheesh, buurrr ',\n",
      "             'info': {},\n",
      "             'time': 'am 31.08.2015 06:46',\n",
      "             'title': '@JW Goethe'}],\n",
      "  'time': 'am 31.08.2015 06:46',\n",
      "  'title': '@JW Goethe'}]\n"
     ]
    }
   ],
   "source": [
    "from urllib2 import urlopen\n",
    "import pprint\n",
    "from HTMLParser import HTMLParser\n",
    "\n",
    "def get_comment_par(attrs):\n",
    "    d = {}\n",
    "    for attr in attrs:\n",
    "        if attr[0] == 'id':\n",
    "            tmp = attr[1].split('_')\n",
    "            d['thread'] = tmp[0][6:]\n",
    "            d['msg'] = tmp[1][3:]\n",
    "        if attr[0] == \"data-voteup\" or attr[0] == \"data-votedown\":\n",
    "            d[attr[0]] = attr[1]\n",
    "    return d\n",
    "\n",
    "\n",
    "class LinksExtractor(HTMLParser): # derive new HTML parser\n",
    "\n",
    "    def __init__(self) :        # class constructor\n",
    "        HTMLParser.__init__(self)  # base class constructor\n",
    "        self.comments = []        # create an empty list for storing hyperlinks\n",
    "        self.insidecomment = False\n",
    "        self.intitle = False\n",
    "        self.inauthor = False\n",
    "        self.intime = False\n",
    "        self.incontent = False\n",
    "        self.inentry = False\n",
    "        self.inreplies = False\n",
    "        self.info = None\n",
    "        self.title = \"\"\n",
    "        self.content = \"\"\n",
    "        self.author =\"\"\n",
    "        self.time = \"\"\n",
    "        self.comment = None \n",
    "        self.replies = None\n",
    "        self.divlevel = 0\n",
    "        self.lilevel = 0\n",
    "        \n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'li':\n",
    "            self.lilevel += 1\n",
    "            for attr in attrs:\n",
    "                if attr[0] == 'class':\n",
    "                    if attr[1] == 'comment':\n",
    "                        self.insidecomment = True\n",
    "                        self.comment = {}\n",
    "                        self.info = get_comment_par(attrs)\n",
    "                        \n",
    "        elif tag == 'h3':\n",
    "            for attr in attrs:\n",
    "                if attr[0] == 'class':\n",
    "                    if attr[1] == 'title':\n",
    "                        self.intitle = True\n",
    "        elif tag == 'span':\n",
    "            for attr in attrs:\n",
    "                if attr[0] == 'class':\n",
    "                    if attr[1] == 'author':\n",
    "                        self.inauthor = True\n",
    "                    elif attr[1] == 'time': \n",
    "                        self.intime = True\n",
    "        elif tag == 'p':\n",
    "            for attr in attrs:\n",
    "                if attr[0] == 'class':\n",
    "                    if attr[1] == 'content':\n",
    "                        self.incontent = True\n",
    "        elif tag == 'div':\n",
    "            self.divlevel += 1\n",
    "            for attr in attrs:\n",
    "                if attr[0] == 'class':\n",
    "                    if attr[1] == 'entry':\n",
    "                        self.inentry = True\n",
    "                    elif 'replies' in attr[1]:\n",
    "                        self.replies =[]\n",
    "                        self.inreplies = True\n",
    "                        self.inentry = False\n",
    "                        \n",
    "            \n",
    "            \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'li':\n",
    "            self.lilevel -= 1\n",
    "            if self.lilevel == 0:\n",
    "                if self.insidecomment:\n",
    "                    print \"add comment\"\n",
    "                    self.comments.append(self.comment)\n",
    "                self.insidecomment = False\n",
    "        elif tag == 'h3':\n",
    "            self.intitle = False\n",
    "        elif tag == 'span':\n",
    "            self.intime = False\n",
    "            self.inauthor = False\n",
    "        elif tag == 'p':\n",
    "            # collect all comment information\n",
    "            \n",
    "            if self.incontent:\n",
    "                if self.inentry:\n",
    "                    print \"collect comment, aut: \"+self.author\n",
    "                    self.comment['info'] = self.info\n",
    "                    self.info = {}\n",
    "                    self.comment['author'] = self.author\n",
    "                    self.comment['time'] = self.time\n",
    "                    self.comment['title'] = self.title\n",
    "                    self.comment['content'] = self.content\n",
    "                    \n",
    "                if self.inreplies:\n",
    "                    print \"save reply: autor: \"+self.author\n",
    "                    col ={}\n",
    "                    col['info'] = self.info\n",
    "                    self.info = {}\n",
    "                    col['author'] = self.author\n",
    "                    col['time'] = self.time\n",
    "                    col['title'] = self.title\n",
    "                    col['content'] = self.content\n",
    "                    self.replies.append(col)\n",
    "            self.incontent = False\n",
    "        elif tag == 'div':\n",
    "            self.divlevel -= 1\n",
    "            if self.divlevel == 0:\n",
    "                if self.inreplies:\n",
    "                    print \"add replies\"\n",
    "                    self.comment['reply'] = self.replies\n",
    "                self.inentry = False\n",
    "                self.inreplies = False\n",
    "        return\n",
    "        \n",
    "    def handle_data(self, data):\n",
    "        if self.intitle:\n",
    "            self.title = data\n",
    "        elif self.inauthor:\n",
    "            self.author = data\n",
    "        elif self.intime:\n",
    "            self.time = data\n",
    "        elif self.incontent:\n",
    "            self.content = data\n",
    "        return\n",
    "    \n",
    "    def get_comments(self) :     # return the list of extracted links\n",
    "        return self.comments\n",
    "\n",
    "\n",
    "\n",
    "zeitung = 'http://www.20min.ch';\n",
    "rubriken = ['schweiz'];\n",
    "\n",
    "storyid = '18530412'\n",
    "kommentare = zeitung + '/community/storydiscussion/messageoverview.tmpl?storyid=' + storyid\n",
    "print kommentare\n",
    "\n",
    "response = urlopen(kommentare,timeout=3)\n",
    "html = response.read()\n",
    "#pprint.pprint(html)\n",
    "\n",
    "\n",
    "p = LinksExtractor()\n",
    "p.feed(html) \n",
    "pprint.pprint(p.get_comments())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Better Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib2 import urlopen\n",
    "import pprint\n",
    "from HTMLParser import HTMLParser\n",
    "import json\n",
    "\n",
    "def taghasattr(key,value,attrs):\n",
    "    for attr in attrs:\n",
    "        if attr[0] == key:\n",
    "            if value in attr[1]:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def get_comment_par(attrs):\n",
    "    d = {}\n",
    "    for attr in attrs:\n",
    "        if attr[0] == 'id':\n",
    "            tmp = attr[1].split('_')\n",
    "            d['thread'] = tmp[0][6:]\n",
    "            d['msg'] = tmp[1][3:]\n",
    "        if attr[0] == \"data-voteup\" or attr[0] == \"data-votedown\":\n",
    "            d[attr[0]] = attr[1]\n",
    "    return d\n",
    "\n",
    "\n",
    "class LinksExtractor(HTMLParser): # derive new HTML parser\n",
    "\n",
    "    def __init__(self) :        # class constructor\n",
    "        HTMLParser.__init__(self)  # base class constructor\n",
    "        self.comments = []        # create an empty list for storing hyperlinks\n",
    "        self.insidecomment = False\n",
    "        self.inreplies = False\n",
    "        self.inentry = False\n",
    "        self.position = \"start\"\n",
    "        self.info = None\n",
    "        self.commentinfo = None\n",
    "        self.title = \"\"\n",
    "        self.content = \"\"\n",
    "        self.author =\"\"\n",
    "        self.time = \"\"\n",
    "        self.comment = None \n",
    "        self.replies = None\n",
    "        self.divlevel = 0\n",
    "        self.lilevel = 0\n",
    "        \n",
    "        \n",
    "    def handle_starttag(self, tag, attrs): \n",
    "        if tag == 'li':\n",
    "            self.lilevel += 1\n",
    "            if taghasattr('class','comment',attrs):\n",
    "                if self.lilevel == 1:\n",
    "                    self.insidecomment = True\n",
    "                    self.comment = {}\n",
    "                    self.info = get_comment_par(attrs)\n",
    "                elif self.lilevel == 2:\n",
    "                    self.commentinfo = get_comment_par(attrs)\n",
    "                        \n",
    "        elif tag == 'h3':\n",
    "            if taghasattr('class','title',attrs):\n",
    "                self.position = 'title'\n",
    "        elif tag == 'span':\n",
    "            if taghasattr('class','author',attrs):\n",
    "                self.position = 'author'\n",
    "            elif taghasattr('class','time',attrs):\n",
    "                self.position = 'time'\n",
    "        \n",
    "        elif tag == 'p':\n",
    "            if taghasattr('class','content',attrs):\n",
    "                self.position = 'content'\n",
    "        elif tag == 'div':\n",
    "            self.divlevel += 1\n",
    "            if taghasattr('class','entry',attrs) and self.divlevel == 1:\n",
    "                self.inentry = True\n",
    "            elif taghasattr('class','replies',attrs):\n",
    "                self.replies =[]\n",
    "                self.inreplies = True\n",
    "                    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'li':\n",
    "            self.lilevel -= 1\n",
    "            if self.lilevel == 0:\n",
    "                if self.insidecomment:\n",
    "                    self.comments.append(self.comment)\n",
    "                self.insidecomment = False\n",
    "        elif tag == 'h3':\n",
    "            self.position = 'out'\n",
    "        elif tag == 'span':\n",
    "            self.position = 'out'\n",
    "            self.position = 'out'\n",
    "        elif tag == 'p':\n",
    "            if self.position == 'content':\n",
    "                col ={}\n",
    "                col['author'] = self.author\n",
    "                col['time'] = self.time\n",
    "                col['title'] = self.title\n",
    "                col['content'] = self.content\n",
    "                if self.inentry:\n",
    "                    col['info'] = self.info\n",
    "                    self.comment = col\n",
    "                if self.inreplies:\n",
    "                    col['replyinfo'] = self.commentinfo\n",
    "                    self.replies.append(col)\n",
    "                self.position = 'out'\n",
    "                \n",
    "        elif tag == 'div':\n",
    "            self.divlevel -= 1\n",
    "            if self.divlevel == 0:\n",
    "                if self.inreplies:\n",
    "                    self.comment['reply'] = self.replies\n",
    "                self.inentry = False\n",
    "                self.inreplies = False\n",
    "        return\n",
    "        \n",
    "    def handle_data(self, data):\n",
    "        if self.position == 'title':\n",
    "            self.title = data\n",
    "        elif self.position == 'author':\n",
    "            self.author = data\n",
    "        elif self.position == 'time':\n",
    "            self.time = data\n",
    "        elif self.position == 'content':\n",
    "            self.content = data\n",
    "        return\n",
    "    \n",
    "    def get_comments(self) :     # return the list of extracted links\n",
    "        return self.comments\n",
    "\n",
    "zeitung = 'http://www.20min.ch';\n",
    "rubriken = ['schweiz'];\n",
    "\n",
    "storyid = '18530412'\n",
    "kommentare = zeitung + '/community/storydiscussion/messageoverview.tmpl?storyid=' + storyid\n",
    "\n",
    "\n",
    "response = urlopen(kommentare,timeout=3)\n",
    "html = response.read()\n",
    "\n",
    "\n",
    "\n",
    "p = LinksExtractor()\n",
    "p.feed(html) \n",
    "\n",
    "comments_s = p.get_comments()\n",
    "#pprint.pprint(comments_s)\n",
    "out_file = open(\"20min.json\",\"w\")\n",
    "\n",
    "# Save the dictionary into this file\n",
    "# (the 'indent=4' is optional, but makes it more readable)\n",
    "json.dump(comments_s,out_file, indent=4)                                    \n",
    "\n",
    "# Close the file\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import json\n",
    "import pprint\n",
    "\n",
    "in_file = open(\"20min.json\",\"r\")\n",
    "article_comments = json.load(in_file)\n",
    "in_file.close()\n",
    "\n",
    "## pprint.pprint(jsonartikel)\n",
    "\n",
    "def count_comment_entries(cm):\n",
    "    count = 0\n",
    "    for entry in cm:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def count_all_entries(cm):\n",
    "    count = 0\n",
    "    for entry in cm:\n",
    "        count += 1\n",
    "        if 'reply' in entry:\n",
    "            for reply in entry['reply']:\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def author_hist(cm):\n",
    "    d = {}\n",
    "    for entry in cm:\n",
    "        if entry['author'] in d:\n",
    "            d[entry['author']] +=1\n",
    "        else:\n",
    "            d[entry['author']] =1\n",
    "            \n",
    "        if 'reply' in entry:\n",
    "            for reply in entry['reply']:\n",
    "                if entry['author'] in d:\n",
    "                    d[entry['author']] +=1\n",
    "                else:\n",
    "                    d[entry['author']] =1\n",
    "    return d\n",
    "    \n",
    "    \n",
    "def reply_hist(cm):\n",
    "    d = []\n",
    "    for entry in cm:\n",
    "        count = 0\n",
    "        if 'reply' in entry:\n",
    "            for rep in entry['reply']:\n",
    "                count += 1\n",
    "        d.append(count)\n",
    "    return d\n",
    "\n",
    "def vote_hist(cm):\n",
    "    vup = []\n",
    "    vdown = []\n",
    "    for entry in cm:\n",
    "        up = int(entry['info']['data-voteup'])\n",
    "        down = int(entry['info']['data-votedown'])\n",
    "        vup.append(up)\n",
    "        vdown.append(down)\n",
    "    return vup,vdown\n",
    "    \n",
    "\n",
    "def nr_of_authors(cm):\n",
    "    return len(author_hist(cm))\n",
    "\n",
    "def author_with_max_comments(cm):\n",
    "    hist = author_hist(cm)\n",
    "    return max(hist,key=hist.get)\n",
    "\n",
    "def get_max_nr_per_author(cm):\n",
    "    hist = author_hist(cm)\n",
    "    return hist[max(hist,key=hist.get)]\n",
    "\n",
    "def max_replies(cm):\n",
    "    return max(reply_hist(cm))\n",
    "\n",
    "def max_vote_up_down(cm):\n",
    "    vu,vd = vote_hist(cm)\n",
    "    return max(vu),max(vd)\n",
    "\n",
    "\"\"\"\n",
    "print count_all_entries(article_comments)\n",
    "print author_hist(article_comments)\n",
    "print nr_of_authors(article_comments)\n",
    "print author_with_max_comments(article_comments)\n",
    "print get_max_nr_per_author(article_comments)\n",
    "print count_comment_entries(article_comments)\n",
    "print reply_hist(article_comments)\n",
    "print max_replies(article_comments)\n",
    "vu,vd = vote_hist(article_comments)\n",
    "print vu\n",
    "print vd\n",
    "print max_vote_up_down(article_comments)\n",
    "\"\"\"\n",
    "print \"Artikel-Fingerprint\"\n",
    "print \"Artikel ID: %s\"%\"xxxx\"\n",
    "print \"Anzahl Kommentare und Antworten: %d\"% count_all_entries(article_comments)\n",
    "print \"Anzahl Kommentare: %d\"% count_comment_entries(article_comments)\n",
    "print \"Anzahl verschiedener Autoren: %d\" % nr_of_authors(article_comments)\n",
    "print \"Max Beiträge eines Autors: %d\" % get_max_nr_per_author(article_comments)\n",
    "print \"Max Rückmeldungen auf einen Kommentar: %d\" % max_replies(article_comments)\n",
    "vu,vd = max_vote_up_down(article_comments)\n",
    "print \"Max Upvotes eines Kommentars: %d\" % vu\n",
    "print \"Max Downvostes eines Kommentars: %d\" % vd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Artikel Charakterisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artikel-Fingerprint\n",
      "Titel: Die Leser nehmen Abschied vom Sommer\n",
      "Artikel ID: 18322505\n",
      "URL: http://www.20min.ch/schweiz/news/story/18322505\n",
      "Anzahl Kommentare und Antworten: 157\n",
      "Anzahl Kommentare: 113\n",
      "Anzahl verschiedener Autoren: 111\n",
      "Max Beiträge eines Autors: 4\n",
      "Max Rückmeldungen auf einen Kommentar: 3\n",
      "Max Up_Votes eines Kommentars: 373\n",
      "Max Down_Votes eines Kommentars: 131\n"
     ]
    }
   ],
   "source": [
    "from urllib2 import urlopen\n",
    "import pprint\n",
    "from HTMLParser import HTMLParser\n",
    "import json\n",
    "\n",
    "def taghasattr(key,value,attrs):\n",
    "    for attr in attrs:\n",
    "        if attr[0] == key:\n",
    "            if value in attr[1]:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def get_comment_par(attrs):\n",
    "    d = {}\n",
    "    for attr in attrs:\n",
    "        if attr[0] == 'id':\n",
    "            tmp = attr[1].split('_')\n",
    "            d['thread'] = tmp[0][6:]\n",
    "            d['msg'] = tmp[1][3:]\n",
    "        if attr[0] == \"data-voteup\" or attr[0] == \"data-votedown\":\n",
    "            d[attr[0]] = attr[1]\n",
    "    return d\n",
    "\n",
    "\n",
    "class ArticleParser(HTMLParser): # derive new HTML parser\n",
    "\n",
    "    def __init__(self) :        # class constructor\n",
    "        HTMLParser.__init__(self)  # base class constructor\n",
    "        self.comments = []        # create an empty list for storing hyperlinks\n",
    "        self.insidecomment = False\n",
    "        self.inreplies = False\n",
    "        self.inentry = False\n",
    "        self.instroytitle = False\n",
    "        self.position = \"start\"\n",
    "        self.info = None\n",
    "        self.commentinfo = None\n",
    "        self.title = \"\"\n",
    "        self.content = \"\"\n",
    "        self.author =\"\"\n",
    "        self.time = \"\"\n",
    "        self.storytitle = \"\"\n",
    "        self.comment = None \n",
    "        self.replies = None\n",
    "        self.divlevel = 0\n",
    "        self.lilevel = 0\n",
    "        \n",
    "        \n",
    "    def handle_starttag(self, tag, attrs): \n",
    "        if tag == 'li':\n",
    "            self.lilevel += 1\n",
    "            if taghasattr('class','comment',attrs):\n",
    "                if self.lilevel == 1:\n",
    "                    self.insidecomment = True\n",
    "                    self.comment = {}\n",
    "                    self.info = get_comment_par(attrs)\n",
    "                elif self.lilevel == 2:\n",
    "                    self.commentinfo = get_comment_par(attrs)\n",
    "                        \n",
    "        elif tag == 'h3':\n",
    "            if taghasattr('class','title',attrs):\n",
    "                self.position = 'title'\n",
    "        elif tag == 'span':\n",
    "            if taghasattr('class','author',attrs):\n",
    "                self.position = 'author'\n",
    "            elif taghasattr('class','time',attrs):\n",
    "                self.position = 'time'\n",
    "            elif self.instroytitle:\n",
    "                self.position = 'storytitle'\n",
    "        \n",
    "        elif tag == 'p':\n",
    "            if taghasattr('class','content',attrs):\n",
    "                self.position = 'content'\n",
    "        elif tag == 'div':\n",
    "            self.divlevel += 1\n",
    "            if taghasattr('class','entry',attrs) and self.divlevel == 1:\n",
    "                self.inentry = True\n",
    "            elif taghasattr('class','replies',attrs):\n",
    "                self.replies =[]\n",
    "                self.inreplies = True\n",
    "            elif taghasattr('class','story_titles',attrs):\n",
    "                self.instroytitle = True\n",
    "                    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'li':\n",
    "            self.lilevel -= 1\n",
    "            if self.lilevel == 0:\n",
    "                if self.insidecomment:\n",
    "                    self.comments.append(self.comment)\n",
    "                self.insidecomment = False\n",
    "        elif tag == 'h3':\n",
    "            self.position = 'out'\n",
    "        elif tag == 'span':\n",
    "            self.position = 'out'\n",
    "            self.position = 'out'\n",
    "        elif tag == 'p':\n",
    "            if self.position == 'content':\n",
    "                col ={}\n",
    "                col['author'] = self.author\n",
    "                col['time'] = self.time\n",
    "                col['title'] = self.title\n",
    "                col['content'] = self.content\n",
    "                if self.inentry:\n",
    "                    col['info'] = self.info\n",
    "                    self.comment = col\n",
    "                if self.inreplies:\n",
    "                    col['replyinfo'] = self.commentinfo\n",
    "                    self.replies.append(col)\n",
    "                self.position = 'out'\n",
    "                \n",
    "        elif tag == 'div':\n",
    "            self.divlevel -= 1\n",
    "            if self.divlevel == 0:\n",
    "                if self.inreplies:\n",
    "                    self.comment['reply'] = self.replies\n",
    "                self.inentry = False\n",
    "                self.inreplies = False\n",
    "            elif self.divlevel < 7 and self.instroytitle:\n",
    "                self.instroytitle = False\n",
    "        return\n",
    "        \n",
    "    def handle_data(self, data):\n",
    "        if self.position == 'title':\n",
    "            self.title = data\n",
    "        elif self.position == 'author':\n",
    "            self.author = data\n",
    "        elif self.position == 'time':\n",
    "            self.time = data\n",
    "        elif self.position == 'content':\n",
    "            self.content = data\n",
    "        elif self.position == 'storytitle':\n",
    "            self.storytitle = data\n",
    "        return\n",
    "    \n",
    "    def get_comments(self) :     # return the list of extracted links\n",
    "        return self.comments\n",
    "    \n",
    "    def get_storytitle(self):\n",
    "        return self.storytitle\n",
    "\n",
    "zeitung = 'http://www.20min.ch';\n",
    "#rubriken = ['schweiz'];\n",
    "\n",
    "### Rubrik Schweiz\n",
    "storyid = '18322505'\n",
    "kommentare = zeitung + '/community/storydiscussion/messageoverview.tmpl?storyid=' + storyid\n",
    "story = zeitung + '/schweiz/news/story/' + storyid\n",
    "\n",
    "\n",
    "# Titel des Artikels zur Storyid\n",
    "response = urlopen(story,timeout=3)\n",
    "html = response.read()\n",
    "# Parse\n",
    "p = ArticleParser()\n",
    "p.feed(html)\n",
    "storytitle = p.get_storytitle()\n",
    "\n",
    "\n",
    "### Kommentare anschauen\n",
    "response = urlopen(kommentare,timeout=3)\n",
    "html = response.read()\n",
    "\n",
    "\n",
    "# Parse\n",
    "p = ArticleParser()\n",
    "p.feed(html) \n",
    "\n",
    "# Article Dictionary / JSON\n",
    "article_comments = p.get_comments()\n",
    "\n",
    "def count_comment_entries(cm):\n",
    "    count = 0\n",
    "    for entry in cm:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def count_all_entries(cm):\n",
    "    count = 0\n",
    "    for entry in cm:\n",
    "        count += 1\n",
    "        if 'reply' in entry:\n",
    "            for reply in entry['reply']:\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def author_hist(cm):\n",
    "    d = {}\n",
    "    for entry in cm:\n",
    "        if entry['author'] in d:\n",
    "            d[entry['author']] +=1\n",
    "        else:\n",
    "            d[entry['author']] =1\n",
    "            \n",
    "        if 'reply' in entry:\n",
    "            for reply in entry['reply']:\n",
    "                if entry['author'] in d:\n",
    "                    d[entry['author']] +=1\n",
    "                else:\n",
    "                    d[entry['author']] =1\n",
    "    return d\n",
    "    \n",
    "    \n",
    "def reply_hist(cm):\n",
    "    d = []\n",
    "    for entry in cm:\n",
    "        count = 0\n",
    "        if 'reply' in entry:\n",
    "            for rep in entry['reply']:\n",
    "                count += 1\n",
    "        d.append(count)\n",
    "    return d\n",
    "\n",
    "def vote_hist(cm):\n",
    "    vup = []\n",
    "    vdown = []\n",
    "    for entry in cm:\n",
    "        up = int(entry['info']['data-voteup'])\n",
    "        down = int(entry['info']['data-votedown'])\n",
    "        vup.append(up)\n",
    "        vdown.append(down)\n",
    "    return vup,vdown\n",
    "    \n",
    "\n",
    "def nr_of_authors(cm):\n",
    "    return len(author_hist(cm))\n",
    "\n",
    "def author_with_max_comments(cm):\n",
    "    hist = author_hist(cm)\n",
    "    return max(hist,key=hist.get)\n",
    "\n",
    "def get_max_nr_per_author(cm):\n",
    "    hist = author_hist(cm)\n",
    "    return hist[max(hist,key=hist.get)]\n",
    "\n",
    "def max_replies(cm):\n",
    "    return max(reply_hist(cm))\n",
    "\n",
    "def max_vote_up_down(cm):\n",
    "    vu,vd = vote_hist(cm)\n",
    "    return max(vu),max(vd)\n",
    "\n",
    "\n",
    "print \"Artikel-Fingerprint\"\n",
    "print \"Titel: %s\"%storytitle\n",
    "print \"Artikel ID: %s\"%storyid\n",
    "print \"URL: %s\"%story\n",
    "print \"Anzahl Kommentare und Antworten: %d\"% count_all_entries(article_comments)\n",
    "print \"Anzahl Kommentare: %d\"% count_comment_entries(article_comments)\n",
    "print \"Anzahl verschiedener Autoren: %d\" % nr_of_authors(article_comments)\n",
    "print \"Max Beiträge eines Autors: %d\" % get_max_nr_per_author(article_comments)\n",
    "print \"Max Rückmeldungen auf einen Kommentar: %d\" % max_replies(article_comments)\n",
    "vu,vd = max_vote_up_down(article_comments)\n",
    "print \"Max Up_Votes eines Kommentars: %d\" % vu\n",
    "print \"Max Down_Votes eines Kommentars: %d\" % vd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.20min.ch/talkbacks/story/25287962\n",
      "Artikel-Fingerprint\n",
      "Titel: Anita Fetz vor letzter Schlacht gegen die Bauern\n",
      "Artikel ID: 25287962\n",
      "URL: http://www.20min.ch/schweiz/news/story/25287962\n",
      "Anzahl Kommentare und Antworten: 27\n",
      "Anzahl Kommentare: 16\n",
      "Anzahl verschiedener Autoren: 16\n",
      "Max Beiträge eines Autors: 3\n",
      "Max Rückmeldungen auf einen Kommentar: 2\n",
      "Max Up_Votes eines Kommentars: 188\n",
      "Max Down_Votes eines Kommentars: 131\n"
     ]
    }
   ],
   "source": [
    "from urllib2 import urlopen\n",
    "import pprint\n",
    "from HTMLParser import HTMLParser\n",
    "import json\n",
    "\n",
    "def taghasattr(key,value,attrs):\n",
    "    for attr in attrs:\n",
    "        if attr[0] == key:\n",
    "            if value in attr[1]:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def get_comment_par(attrs):\n",
    "    d = {}\n",
    "    for attr in attrs:\n",
    "        if attr[0] == 'id':\n",
    "            tmp = attr[1].split('_')\n",
    "            d['thread'] = tmp[0][6:]\n",
    "            d['msg'] = tmp[1][3:]\n",
    "        if attr[0] == \"data-voteup\" or attr[0] == \"data-votedown\":\n",
    "            d[attr[0]] = attr[1]\n",
    "    return d\n",
    "\n",
    "\n",
    "class ArticleParser(HTMLParser): # derive new HTML parser\n",
    "\n",
    "    def __init__(self) :        # class constructor\n",
    "        HTMLParser.__init__(self)  # base class constructor\n",
    "        self.comments = []        # create an empty list for storing hyperlinks\n",
    "        self.insidecomment = False\n",
    "        self.inreplies = False\n",
    "        self.inentry = False\n",
    "        self.instroytitle = False\n",
    "        self.position = \"start\"\n",
    "        self.info = None\n",
    "        self.commentinfo = None\n",
    "        self.title = \"\"\n",
    "        self.content = \"\"\n",
    "        self.author =\"\"\n",
    "        self.time = \"\"\n",
    "        self.storytitle = \"\"\n",
    "        self.comment = None \n",
    "        self.replies = None\n",
    "        self.divlevel = 0\n",
    "        self.lilevel = 0\n",
    "        \n",
    "        \n",
    "    def handle_starttag(self, tag, attrs): \n",
    "        if tag == 'li':\n",
    "            self.lilevel += 1\n",
    "            if taghasattr('class','comment',attrs):\n",
    "                if self.lilevel == 1:\n",
    "                    self.insidecomment = True\n",
    "                    self.comment = {}\n",
    "                    self.info = get_comment_par(attrs)\n",
    "                elif self.lilevel == 2:\n",
    "                    self.commentinfo = get_comment_par(attrs)\n",
    "                    \n",
    "                        \n",
    "        elif tag == 'h3':\n",
    "            if taghasattr('class','title',attrs):\n",
    "                self.position = 'title'\n",
    "        elif tag == 'span':\n",
    "            if taghasattr('class','author',attrs):\n",
    "                self.position = 'author'\n",
    "            elif taghasattr('class','time',attrs):\n",
    "                self.position = 'time'\n",
    "            elif self.instroytitle:\n",
    "                self.position = 'storytitle'\n",
    "        \n",
    "        elif tag == 'p':\n",
    "            if taghasattr('class','content',attrs):\n",
    "                self.position = 'content'\n",
    "        elif tag == 'div':\n",
    "            self.divlevel += 1\n",
    "            if taghasattr('class','entry',attrs) and self.divlevel < 9:\n",
    "                self.inentry = True\n",
    "            elif taghasattr('class','replies',attrs):\n",
    "                self.replies =[]\n",
    "                self.inreplies = True\n",
    "            elif taghasattr('class','story_titles',attrs):\n",
    "                self.instroytitle = True\n",
    "                    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'li':\n",
    "            self.lilevel -= 1\n",
    "            if self.lilevel == 0:\n",
    "                if self.insidecomment:\n",
    "                    self.comments.append(self.comment)\n",
    "                self.insidecomment = False\n",
    "        elif tag == 'h3':\n",
    "            self.position = 'out'\n",
    "        elif tag == 'span':\n",
    "            self.position = 'out'\n",
    "            self.position = 'out'\n",
    "        elif tag == 'p':\n",
    "            if self.position == 'content':\n",
    "                col ={}\n",
    "                col['author'] = self.author\n",
    "                col['time'] = self.time\n",
    "                col['title'] = self.title\n",
    "                col['content'] = self.content\n",
    "                if self.inentry:\n",
    "                    col['info'] = self.info\n",
    "                    self.comment = col\n",
    "                if self.inreplies:\n",
    "                    col['replyinfo'] = self.commentinfo\n",
    "                    self.replies.append(col)\n",
    "                self.position = 'out'\n",
    "                \n",
    "        elif tag == 'div':\n",
    "            self.divlevel -= 1\n",
    "            if self.divlevel < 8 and self.comment != None:\n",
    "                if self.inreplies:\n",
    "                    self.comment['reply'] = self.replies\n",
    "                self.inentry = False\n",
    "                self.inreplies = False\n",
    "            elif self.divlevel < 7 and self.instroytitle:\n",
    "                self.instroytitle = False\n",
    "        return\n",
    "        \n",
    "    def handle_data(self, data):\n",
    "        if self.position == 'title':\n",
    "            self.title = data\n",
    "        elif self.position == 'author':\n",
    "            self.author = data\n",
    "        elif self.position == 'time':\n",
    "            self.time = data\n",
    "        elif self.position == 'content':\n",
    "            self.content = data\n",
    "        elif self.position == 'storytitle':\n",
    "            self.storytitle = data\n",
    "        return\n",
    "    \n",
    "    def get_comments(self) :     # return the list of extracted links\n",
    "        return self.comments\n",
    "    \n",
    "    def get_storytitle(self):\n",
    "        return self.storytitle\n",
    "\n",
    "zeitung = 'http://www.20min.ch';\n",
    "#rubriken = ['schweiz'];\n",
    "\n",
    "### Rubrik Schweiz\n",
    "storyid = '25287962'\n",
    "#kommentare = zeitung + '/community/storydiscussion/messageoverview.tmpl?storyid=' + storyid\n",
    "story = zeitung + '/schweiz/news/story/' + storyid\n",
    "kommentare = zeitung + '/talkbacks/story/' + storyid\n",
    "print kommentare\n",
    "\n",
    "# Titel des Artikels zur Storyid\n",
    "response = urlopen(story,timeout=3)\n",
    "html = response.read()\n",
    "# Parse\n",
    "p = ArticleParser()\n",
    "p.feed(html)\n",
    "storytitle = p.get_storytitle()\n",
    "\n",
    "\n",
    "### Kommentare anschauen\n",
    "response = urlopen(kommentare,timeout=3)\n",
    "html = response.read()\n",
    "\n",
    "\n",
    "# Parse\n",
    "p = ArticleParser()\n",
    "p.feed(html) \n",
    "\n",
    "# Article Dictionary / JSON\n",
    "article_comments = p.get_comments()\n",
    "#pprint.pprint(article_comments)\n",
    "\n",
    "def count_comment_entries(cm):\n",
    "    count = 0\n",
    "    for entry in cm:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def count_all_entries(cm):\n",
    "    count = 0\n",
    "    for entry in cm:\n",
    "        count += 1\n",
    "        if 'reply' in entry:\n",
    "            for reply in entry['reply']:\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def author_hist(cm):\n",
    "    d = {}\n",
    "    for entry in cm:\n",
    "        if entry['author'] in d:\n",
    "            d[entry['author']] +=1\n",
    "        else:\n",
    "            d[entry['author']] =1\n",
    "            \n",
    "        if 'reply' in entry:\n",
    "            for reply in entry['reply']:\n",
    "                if entry['author'] in d:\n",
    "                    d[entry['author']] +=1\n",
    "                else:\n",
    "                    d[entry['author']] =1\n",
    "    return d\n",
    "    \n",
    "    \n",
    "def reply_hist(cm):\n",
    "    d = []\n",
    "    for entry in cm:\n",
    "        count = 0\n",
    "        if 'reply' in entry:\n",
    "            for rep in entry['reply']:\n",
    "                count += 1\n",
    "        d.append(count)\n",
    "    return d\n",
    "\n",
    "def vote_hist(cm):\n",
    "    vup = []\n",
    "    vdown = []\n",
    "    for entry in cm:\n",
    "        up = int(entry['info']['data-voteup'])\n",
    "        down = int(entry['info']['data-votedown'])\n",
    "        vup.append(up)\n",
    "        vdown.append(down)\n",
    "    return vup,vdown\n",
    "    \n",
    "\n",
    "def nr_of_authors(cm):\n",
    "    return len(author_hist(cm))\n",
    "\n",
    "def author_with_max_comments(cm):\n",
    "    hist = author_hist(cm)\n",
    "    return max(hist,key=hist.get)\n",
    "\n",
    "def get_max_nr_per_author(cm):\n",
    "    hist = author_hist(cm)\n",
    "    return hist[max(hist,key=hist.get)]\n",
    "\n",
    "def max_replies(cm):\n",
    "    return max(reply_hist(cm))\n",
    "\n",
    "def max_vote_up_down(cm):\n",
    "    vu,vd = vote_hist(cm)\n",
    "    return max(vu),max(vd)\n",
    "\n",
    "\n",
    "print \"Artikel-Fingerprint\"\n",
    "print \"Titel: %s\"%storytitle\n",
    "print \"Artikel ID: %s\"%storyid\n",
    "print \"URL: %s\"%story\n",
    "print \"Anzahl Kommentare und Antworten: %d\"% count_all_entries(article_comments)\n",
    "print \"Anzahl Kommentare: %d\"% count_comment_entries(article_comments)\n",
    "print \"Anzahl verschiedener Autoren: %d\" % nr_of_authors(article_comments)\n",
    "print \"Max Beiträge eines Autors: %d\" % get_max_nr_per_author(article_comments)\n",
    "print \"Max Rückmeldungen auf einen Kommentar: %d\" % max_replies(article_comments)\n",
    "vu,vd = max_vote_up_down(article_comments)\n",
    "print \"Max Up_Votes eines Kommentars: %d\" % vu\n",
    "print \"Max Down_Votes eines Kommentars: %d\" % vd\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
